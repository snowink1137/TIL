> 코드잇 머신 러닝 강의 중 다시 볼 내용을 정리한 문서
>
> https://www.codeit.kr/learn/courses/machine-learning



[TOC]



# 1. 머신 러닝 기본기

## 1.1. 머신 러닝이란?

### 머신 러닝

- > 기계가 학습한다는 건, 프로그램이 특정 작업(T)을 하는 데 있어서 경험(E)을 통해 작업의 성능(P)을 향상 시키는 것
  >
  > -톰 미첼(Tom Mitchell)-

- Program vs Machine Learning Program

  - 기존 프로그램은 항상 똑같은 결과가 나온다. 머신 러닝 프로그램은 경험을 통해서 스스로 업무 능력이 향상된다



### 학습의 유형

- 지도 학습(Supervised learning)
  - "답"이 있고 이 답을 맞추는 게 학습의 목적
  - 분류(Classfication) 유형
    - 몇 가지 옵션 중 하나를 골라서 결과를 맞추는 유형
  - 회귀(Regression)
    - 결과 값이 무수히 많고 연속적임. 이 결과를 맞추는 유형
- 비지도 학습(Unsupervised learning)
  - "답"이 없고 이 답을 맞추는 게 학습의 목적
  - ex) 프로그램이 정답 없이 "비슷한" 기준대로 묶는 것



### k-NN 알고리즘

- 데이터의 결과를 예측할 때, k 개의 가까운 데이터를 사용해서 예측하는 것
- 이 알고리즘이 머신 러닝인가? 그렇다
  - 데이터가 많아지면 많아질수록(경험↑) kNN 알고리즘의 성능이 향상되니까



### 머신 러닝의 수학

- 선형 대수학
  - 일차식이나 일차 함수를 다루는 학문
  - 행렬
    - 계산을 효율적으로 할 수 있음
- 미적분학
  - 최적화하는 과정에서 사용됨(최소값, 최대값 등)
- 통계
  - 데이터의 큰 흐름 파악
- 확률
  - 가능성을 수치로 계산하는 데 쓰임



## 1.2. 선형 대수학 & 미분

- 행이 하나이거나 열이 하나인 행렬을 벡터로 보면 된다
  - 보통 별 말 없으면 열벡터를 뜻함
- 선형 시스템(일차 다항식 여러 개 있는 것)을 행렬과 벡터로 쉽게 표현할 수 있다
- 머신 러닝을 할 때, 데이터를 일차식에 사용하는 경우가 많다. 행렬을 사용하면 식을 간편하게 정의할 수 있고, 효율적인 계산이 가능하다
- 편미분으로 기울기 벡터를 구할 수 있다
  - 기울기 벡터의 의미는 어떤 방향으로 가야 가장 가파르게 올라갈지 아니면 가장 가파르게 내려갈 지 알 수 있다는 것에 있다
  - 기울기 벡터는 각 변수를 편미분한 결과를 벡터로 모아놓은 것이다
  - ∇f(x, y) = [2x, 4y] 이런 식으로 표시한다
- 머신 러닝에 미분이 필요한 이유
  - 머신 러닝 성능을 평가하는 기준을 함수로 표현하고 해당 함수값을 최적화하는 방식으로 성능을 평가할 수 있기 때문에



# 2. 기본 지도 학습 알고리즘

## 2.1. 선형 회귀(Linear Regression)

- 데이터에 가장 잘 맞는 "최적선(line of best fit)"을 찾는 것
- 지도 학습 알고리즘
- 목표 변수
  - 맞추려고 하는 값(target variable / output variable)
- 입력 변수
  - 맞추는 데 사용하는 값(input variable / feature)



### 가설 함수

- 가설 함수(hypothesis function)
  - 최적선을 찾는 데 쓰이는 함수
  - $h_θ = θ_0 + θ_1x^1 + θ_2x^2 + θ_3x^3 + \cdots$
  - 적절한 세타 값들을 찾는 것이 최적선을 찾는 것이다
    - 학습을 통해 찾아낸 최적선을 모델이라고 부른다



### 평균 제곱 오차(MSE)

- 가설 함수 평가 방법 중 하나
- 오차(가설 함수 값 - 실제 값)를 제곱해서 평균내는 방식
  - 제곱하는 이유
    - 양수 음수 한꺼번에 처리
    - 오차 많이 나는 경우 더 크게 반영하려고



### 손실 함수(loss function)

- 가설 함수를 평가하기 위한 함수
- 선형 회귀의 경우에는 평균 제곱 오차가 손실 함수의 아웃풋임
- ![image-20201129153537588](머신러닝_정리.assets/image-20201129153537588.png)
  - 분모가 2m인 이유는 이후에 계산을 더 편하게 하기 위한 장치라고 함
  - 인풋이 세타인 것 주의



### 경사 하강법(Gradient Descent)

- 손실 함수의 아웃풋을 최소화하는 방향으로 손실 함수의 세타를 조정해 나가는 것
- 기울기 벡터를 이용해서 손실 함수의 극소점을 찾아 나갈 수 있음
  - 해당 방향으로 세타들을 각각 조정해 나가는데, 기울기 만큼 바로 조정하는 것이 아니라 learning rate를 정해서 일정 비율을 반영해서 세타 값들을 조정해 나가는 것이다
- ![image-20201129154423071](머신러닝_정리.assets/image-20201129154423071.png)
  - 주의할 점은 세타0을 먼저 조정한다고 가정할 경우, 세타1을 조정할 때는 기존의 세타0을 사용해야한다는 것이다
- ![image-20201129154540533](머신러닝_정리.assets/image-20201129154540533.png)
- ![image-20201129154844950](머신러닝_정리.assets/image-20201129154844950.png)
  - 위처럼 error 혹은 error * x의 평균을 나타내는 기호로 간단하게 나타낼 수도 있다



### 학습률(learning rate) 알파

- 학습률에 따라서 손실 함수의 최적화가 잘 될 수도, 안 될 수도, 오래 걸릴 수도 있다
- 학습률이 너무 크다면, 극소점을 사이에 두고 왔다 갔다 하다고 오히려 멀어져 버릴 수도 있다
- 학습률이 너무 작다면, 극소점까지 접근하는 iteration이 많이 필요할 수 있다
- 따라서 적당한 학습률을 찾아야 하는데, 이는 여러 개를 실험해보면서 찾아야 한다
  - 일반적으로 1.0 ~ 0.0 사이의 숫자를 정해서 실험한다



### 모델 평가하기

- 평균 제곱근 오차(RMSE, root mean square error)를 많이 씀
  - MSE의 제곱근 값
  - 제곱하면 단위도 제곱되니까 단위를 맞추기 위해서 제곱근을 사용
- 학습과 평가를 위한 데이터를 나눈 후, 학습된 모델에 평가 데이터를 집어 넣어서 평가함
  - 학습에 쓰인 데이터를 집어넣으면 해당 데이터로 학습된 모델이므로 당연히 평가 결과가 좋게 나올 테니까



### scikit-learn

- 머신 러닝에 쓰이는 수식들을 미리 구현해놓은 라이브러리

- ```python
  # ex)
  # 필요한 라이브러리 import
  from sklearn import datasets
  from sklearn.model_selection import train_test_split
  from sklearn.linear_model import LinearRegression
  from sklearn.metrics import mean_squared_error
  
  import pandas as pd  
  
  # 보스턴 집 데이터 갖고 오기
  boston_house_dataset = datasets.load_boston()
  
  # 입력 변수를 사용하기 편하게 pandas dataframe으로 변환
  X = pd.DataFrame(boston_house_dataset.data, columns=boston_house_dataset.feature_names)
  
  # 목표 변수를 사용하기 편하게 pandas dataframe으로 변환
  y = pd.DataFrame(boston_house_dataset.target, columns=['MEDV'])
  
  # 코드를 쓰세요
  X = X[['CRIM']]
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=5)
  linear_regression_model = LinearRegression()
  linear_regression_model.fit(X_train, y_train)
  y_test_predict = linear_regression_model.predict(X_test)
  
  # 테스트 코드 (평균 제곱근 오차로 모델 성능 평가)
  mse = mean_squared_error(y_test, y_test_predict)
  
  mse ** 0.5
  ```



## 2.2. 다중 선형 회귀(Multiple Linear Regression)

### 다중 선형 회귀(경사 하강법 사용)

- 선형 회귀와 개념은 똑같다
- 그냥 선형 회귀에서는 feature를 하나만 썼던 거고, 다중 선형 회귀는 feature를 여러 개를 사용하는 것. 선형이기는 하나 2차원에 나타내지지 않는 것뿐

- 다중 선형 회귀 표현법

  - ![image-20201129173135453](머신러닝_정리.assets/image-20201129173135453.png)
  - i번째 데이터의 j번째 feature를 나타냄

- 가설 함수를 벡터로 간결하게 표현하기

  - ![image-20201129173445740](머신러닝_정리.assets/image-20201129173445740.png)
  - ![image-20201129173459314](머신러닝_정리.assets/image-20201129173459314.png)
  - ![image-20201129173511651](머신러닝_정리.assets/image-20201129173511651.png)

- 다중 선형 회귀에서도 손실 함수에 의한 경사 하강법은 똑같다. 조절해야하는 세타 갯 수가 늘어난다는 것뿐

  - 일반화하면 아래와 같다
  - ![image-20201129173846253](머신러닝_정리.assets/image-20201129173846253.png)
  - 행렬을 사용해서 좀더 간단하게 표현하면 아래와 같다
  - ![image-20201129174104135](머신러닝_정리.assets/image-20201129174104135.png)
    - ![image-20201129174224569](머신러닝_정리.assets/image-20201129174224569.png)
    - ![image-20201129174240533](머신러닝_정리.assets/image-20201129174240533.png)
    - ![image-20201129174301507](머신러닝_정리.assets/image-20201129174301507.png)

- ```python
  # ex) 라이브러리 안쓰고 numpy로만 다중 선형 회귀 구현
  import numpy as np
  
  def prediction(X, theta):
      """다중 선형 회귀 가정 함수. 모든 데이터에 대한 예측 값을 numpy 배열로 리턴한다"""
      # 전 과제 코드를 갖고 오세요
      return X @ theta
  
  def gradient_descent(X, theta, y, iterations, alpha):
      """다중 선형 회귀 경사 하강법을 구현한 함수"""
      m = len(X)  # 입력 변수 개수 저장
      
      for _ in range(iterations):
          # 코드를 쓰세요
          error = prediction(X, theta) - y
          theta = theta - alpha / m * (X.T @ error)
          
      return theta
      
  
  # 입력 변수
  house_size = np.array([1.0, 1.5, 1.8, 5, 2.0, 2.5, 3.0, 3.5, 4.0, 5.0, 6.0, 7.0, 8.0, 8.5, 9.0, 10.0])  # 집 크기
  distance_from_station = np.array([5, 4.6, 4.2, 3.9, 3.9, 3.6, 3.5, 3.4, 2.9, 2.8, 2.7, 2.3, 2.0, 1.8, 1.5, 1.0])  # 지하철역으로부터의 거리 (km)
  number_of_rooms = np.array([1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4])  # 방 수
  
  # 목표 변수
  house_price = np.array([3, 3.2, 3.6 , 8, 3.4, 4.5, 5, 5.8, 6, 6.5, 9, 9, 10, 12, 13, 15])  # 집 가격
  
  # 설계 행렬 X 정의
  X = np.array([
      np.ones(16),
      house_size,
      distance_from_station,
      number_of_rooms
  ]).T
  
  # 입력 변수 y 정의
  y = house_price
  
  # 파라미터 theta 초기화
  theta = np.array([0, 0, 0, 0])
  
  # 학습률 0.01로 100번 경사 하강
  theta = gradient_descent(X, theta, y, 100, 0.01)
  
  theta
  ```



### 다중 선형 회귀(정규 방정식 사용)

- 손실 함수를 최적화할 때, learning rate에 따라 세타 값을 수정해가면서 하는 것이 아니라, 아예 극소점을 방정식으로 풀어서 최적화하는 방식

- ```python
  # ex) 정규 방정식을 사용한 다중 선형 회귀
  import numpy as np
  
  def normal_equation(X, y):
      """설계 행렬 X와 목표 변수 벡터 y를 받아 정규 방정식으로 최적의 theta를 구하는 함수"""
      # 코드를 쓰세요
      return np.linalg.pinv(X.T @ X) @ X.T @ y
      
  # 입력 변수
  house_size = np.array([1.0, 1.5, 1.8, 5, 2.0, 2.5, 3.0, 3.5, 4.0, 5.0, 6.0, 7.0, 8.0, 8.5, 9.0, 10.0])  # 집 크기
  distance_from_station = np.array([5, 4.6, 4.2, 3.9, 3.9, 3.6, 3.5, 3.4, 2.9, 2.8, 2.7, 2.3, 2.0, 1.8, 1.5, 1.0])  # 지하철역으로부터의 거리 (km)
  number_of_rooms = np.array([1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4])  # 방 수
  
  # 목표 변수
  house_price = np.array([3, 3.2, 3.6 , 8, 3.4, 4.5, 5, 5.8, 6, 6.5, 9, 9, 10, 12, 13, 15])  # 집 가격
  
  # 입력 변수 파라미터 X 정의
  X = np.array([
      np.ones(16),
      house_size,
      distance_from_station,
      number_of_rooms
  ]).T
  
  # 입력 변수 y 정의
  y = house_price
  
  # 정규 방정식으로 theta 계산
  theta = normal_equation(X, y)
  theta
  ```



### 경사 하강법 vs 정규 방정식

- 정규 방정식 장점
  - learning rate를 찾을 필요가 없다
  - 반복문을 사용하지 않고, 한번에 극소점을 찾을 수 있다
- 정규 방정식 단점
  - feature 개수가 많아지면 경사 하강법에 비해 계산하는 비용이 너무 커진다
  - 역행렬이 존재하지 않는 경우도 있다. 물론 pseudo inverse를 구해서 계산이 가능하기 때문에 큰 문제는 아니다
- 상황따라 다르지만, 일반적으로 feature 개수가 1000개를 넘어가면 경사 하강법을 사용한다고 한다



### Convex 함수

- 아래로 볼록인 함수
- 손실 함수가 convex 함수가 아니라면, 경사 하강법이나 정규 방정식을 사용해서 얻은 최적화된 세타 값이 손실 함수를 최소화 하는 값인지 확실하게 알 수 없다
- 선형 회귀의 평균 제곱 오차 손실 함수는 항상 convex 함수이기 때문에, 경사 하강법이나 정규 방정식을 사용해서 항상 최적의 세타 값을 구할 수 있다



### scikit-learn 예시

- ```python
  # ex) 다중 선형 회귀 예시
  # 필요한 라이브러리 import
  from sklearn import datasets
  from sklearn.model_selection import train_test_split
  from sklearn.linear_model import LinearRegression
  from sklearn.metrics import mean_squared_error
  
  import pandas as pd  
  
  # 당뇨병 데이터 갖고 오기
  diabetes_dataset = datasets.load_diabetes()
  
  # 입력 변수를 사용하기 편하게 pandas dataframe으로 변환
  X = pd.DataFrame(diabetes_dataset.data, columns=diabetes_dataset.feature_names)
  
  # 목표 변수를 사용하기 편하게 pandas dataframe으로 변환
  y = pd.DataFrame(diabetes_dataset.target, columns=['diabetes'])
  
  # 코드를 쓰세요
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=5)
  
  linear_regression_model = LinearRegression()
  linear_regression_model.fit(X_train, y_train)
  
  y_test_predict = linear_regression_model.predict(X_test)
  
  # 평균 제곱 오차의 루트를 통해서 테스트 데이터에서의 모델 성능 판단
  mse = mean_squared_error(y_test, y_test_predict)
  
  mse ** 0.5
  ```



## 2.3. 다항 회귀(Polynomial Regression)

- 가설 함수를 곡선으로 하는 방법
- 다항 회귀의 의미
  - 선형 회귀에서는 많은 feature를 사용하더라도 각 feature가 독립적이기 때문에 속성 사이의 관계에 대한 정보가 예측에 포함되지 않는다
  - 예를 들어, feature 중 집의 높이와 너비는 서로 같이 커야 집 값이 커질 텐데, 선형 회귀를 사용하면 두 feature의 관계를 학습하지는 못한다. 다만 독립적으로 집 값을 최대화 하는 세타 값들을 찾아낼 뿐
  - feature들을 서로 곱해서 차수를 높여줌으로써 선형 회귀를 다항 회귀 문제로 만들어주면 feature 간의 관계를 프로그램에 학습시킬 수 있다

- ![](머신러닝_정리.assets/polynomial_regression.png)
- 위 처럼 곡선으로 모델을 만들면 좀더 fit한 모델을 만들어 낼 수도 있다



### 단일 속성 다항 회귀

- feature를 하나만 선정해서 이 것의 차수를 높인 후 다중 선형 회귀하듯이 계산해서 세타 값을 구해내면 된다
- ![image-20201129191921982](머신러닝_정리.assets/image-20201129191921982.png)
- 위와 같이 하나의 x에 대해 가설 함수를 만든다. 예시는 3차 다항식으로 만든 가설 함수
  - 이렇게 만들어진 가설 함수는 마치 다중 선형 회귀에서 본 식과 비슷하게 생겼다
  - 따라서 $x, x^2, x^3$을 $x_1, x_2, x_3$처럼 생각하고 다중 선형 회귀에서 본 방식대로 세타 값을 찾는 방식으로 모델을 학습시킬 수 있는 것이다
  - 예를 들어 x를 집 크기라고 한다면 다음과 같이 데이터를 만들어서 다중 선형 회귀 방식으로 세타 값을 찾아내면 된다
  - ![image-20201129192728398](머신러닝_정리.assets/image-20201129192728398.png)
  - ![image-20201129192747864](머신러닝_정리.assets/image-20201129192747864.png)



### 다중 다항 회귀

- feature를 여러 개 선정해서 이 것의 차수를 높인 후 다중 선형 회귀하듯이 계산해서 세타 값을 구해내면 된다
- 단일 속성 다항 회귀와 다른 점은 차수를 높이는 경우의 수가 feature 갯수에 따라 늘어난다는 것이다
- 예를 들어, feature를 3개 선정하고 가설 함수의 차수를 2차로 하기로 했다면 조합에 의한 경우의 수는 6이다
  - ![image-20201129193039108](머신러닝_정리.assets/image-20201129193039108.png)
  - 항의 갯수가 많아지긴 했지만 이것 역시 세타 값을 계산하는 데 있어서는 다중 선형 회귀 하듯이 하면 된다
  - ![image-20201129193200795](머신러닝_정리.assets/image-20201129193200795.png)
  - 위와 같이 데이터를 만들어서 하면 된다



### scikit-learn 예시

- ```python
  # 다중 다항 회귀 예시
  # 필요한 라이브러리 import
  from sklearn import datasets
  from sklearn.preprocessing import PolynomialFeatures
  from sklearn.model_selection import train_test_split
  from sklearn.linear_model import LinearRegression
  from sklearn.metrics import mean_squared_error
  
  import pandas as pd  
  
  diabetes_dataset = datasets.load_diabetes()
  
  # 지난 과제 코드를 가지고 오세요.
  polynomial_transformer = PolynomialFeatures(2)  # 2 차식 변형기를 정의한다
  polynomial_features = polynomial_transformer.fit_transform(diabetes_dataset.data)  # 당뇨 데이터를 2차항 문제로 변환
  
  features = polynomial_transformer.get_feature_names(diabetes_dataset.feature_names)  # 입력 변수 이름들도 맞게 바꾼다
  
  X = pd.DataFrame(polynomial_features, columns=features)
  
  # 목표 변수
  y = pd.DataFrame(diabetes_dataset.target, columns=['diabetes'])
  
  # 코드를 쓰세요
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=5)
  
  linear_regression_model = LinearRegression()
  linear_regression_model.fit(X_train, y_train)
  
  y_test_predict = linear_regression_model.predict(X_test)
  
  mse = mean_squared_error(y_test, y_test_predict)
  mse ** 0.5
  ```



## 2.4. 로지스틱 회귀(Logistic Regression)

### 분류 문제

- 지도 학습 중, 회귀는 연속적인 값 예측, 분류는 정해진 몇 개의 값 중 예측

- 보통 분류 문제는 각 결과 값에 숫자 값을 지정해서 푼다
  - ex) 야구: 0, 축구: 1, 농구: 2
- 선형 회귀 방법으로도 분류를 할 수 있기는 하지만, 선형 회귀로 분류를 하면 예외적인 데이터에 민감하게 반응하기 때문에 잘 사용하지 않는다



### 로지스틱 회귀

- 가설 함수를 만들때, 시그모이드 함수를 사용해서 데이터에 가장 잘 맞는 모델을 만드는 방법
  - ![image-20201129204329838](머신러닝_정리.assets/image-20201129204329838.png)

- 분류 문제를 풀 때 사용한다
  - 분류 문제인데 왜 회귀라는 용어를 사용하나?
    - 결국 시그모이드 함수의 리턴 값도 연속적인 값이기 때문



### 시그모이드 함수

- 0과 1사이의 함수값을 갖는 함수. 예외적인 데이터에 영향을 덜 받기 때문에 분류에 더 적합하다
- cf) 선형 회귀는 결과가 범위 없이 얼마든지 크거나 작아질 수 있음



### 로지스틱 가설 함수

- 선형 회귀에서 썼던 가설 함수를 시그모이드 함수에 합성해서 만든다
  - 가설 함수의 함수 값을 0과 1사이로 제한해서 이를 확률로 해석하고자 하는 것이다
    - 0.5가 넘으면 1로 분류하고, 0.5가 안되면 0으로 분류하는 식으로
    - 강의에서는 가설 함수가 0.9면 90%의 확률이라고 했는데, 수학적으로 그렇게 딱 맞는 것인지는 확인해봐야할 것 같다
  - ![image-20201129205520267](머신러닝_정리.assets/image-20201129205520267.png)
  - ![image-20201129205537641](머신러닝_정리.assets/image-20201129205537641.png)
  - ![image-20201129205549544](머신러닝_정리.assets/image-20201129205549544.png)
  - 결국에 여기서도 학습을 통해 적절한 세타 값을 찾아서 최적화된 모델을 찾는 것이 목적이다
- 세타 값에 따른 가설 함수 변화 ex)
  - 가설 함수에 feature가 1개라고 가정했을 때, 다음과 같다
  - ![image-20201129205912159](머신러닝_정리.assets/image-20201129205912159.png)
  - ![image-20201129205933953](머신러닝_정리.assets/image-20201129205933953.png)
    - x축 평행이동
  - ![image-20201129210011764](머신러닝_정리.assets/image-20201129210011764.png)
    - 곡선 모양 변화



### Decision Boundary

- 분류를 구별하는 경계선

- feature가 한 개인 경우
  - ![image-20201129210215783](머신러닝_정리.assets/image-20201129210215783.png)
  - ![image-20201129210235292](머신러닝_정리.assets/image-20201129210235292.png)
  - 위와 같은 경우 47이라는 값이 Decision Boundary가 된다
- feature가 두 개인 경우
  - 가설함수
    - ![image-20201129210410603](머신러닝_정리.assets/image-20201129210410603.png)
  - ![image-20201129210435255](머신러닝_정리.assets/image-20201129210435255.png)
    - 색으로 결과 정보(통과, 탈락) 표현
  - ![image-20201129210624074](머신러닝_정리.assets/image-20201129210624074.png)
  - 위와 같은 경우 Decision Boundary는 주황색 선이 된다



### 로지스틱 회귀의 손실 함수

- 가설 함수를 평가할 수 있는 손실 함수가 필요하다

- 로그 손실(log-loss / cross entropy)을 사용한다
  - cf) 선형 회귀에서는 MSE(평균 제곱 오차)를 손실 함수로 사용했었다
  - 손실의 정도를 로그 함수를 이용해서 표현하므로 로그 손실이라 부름
- ![image-20201129211212614](머신러닝_정리.assets/image-20201129211212614.png)
  - ![image-20201129211717368](머신러닝_정리.assets/image-20201129211717368.png)
  - 같은 표현임
- ![image-20201129211235299](머신러닝_정리.assets/image-20201129211235299.png)
  - 손실 함수를 만드는 데 쓰는 거니까 함수 값이 작을 수록 잘하고 있는 것임
- 로그 손실을 활용한 로지스틱 회귀 손실 함수
  - 각 데이터에 대해 로그 손실을 구한 후 평균 내는 방식
  - ![image-20201129211908648](머신러닝_정리.assets/image-20201129211908648.png)



### 로지스틱 회귀 경사 하강법

- 손실 함수 및 가설 함수가 다르지만 선형 회귀와 방법은 똑같다
- ![image-20201129212227921](머신러닝_정리.assets/image-20201129212227921.png)
- 이런식으로 세타 값이 3개 있다고 했을 때, 손실 함수를 편미분해서 각 세타 값을 조절해 나가는 것이다

- 행렬을 사용한 표현법

  - ![image-20201129212835781](머신러닝_정리.assets/image-20201129212835781.png)
  - ![image-20201129212856017](머신러닝_정리.assets/image-20201129212856017.png)
  - ![image-20201129212905707](머신러닝_정리.assets/image-20201129212905707.png)
  - ![image-20201129212919000](머신러닝_정리.assets/image-20201129212919000.png)
  - ![image-20201129212927256](머신러닝_정리.assets/image-20201129212927256.png)
  - ![image-20201129213002397](머신러닝_정리.assets/image-20201129213002397.png)
    - y는 목표 변수 벡터
  - ![image-20201129213242099](머신러닝_정리.assets/image-20201129213242099.png)
    - 로지스틱 손실 함수를 편미분 하고나면 마치 선형 회귀의 손실 함수(MSE)를 편미분 했을 때랑 똑같은 표현이 나옴
  - 따라서, 최종적인 표현은 다음과 같다
    - ![image-20201129213220347](머신러닝_정리.assets/image-20201129213220347.png)

- ```python
  # ex) numpy로만 구현한 예시
  import numpy as np
  
  def sigmoid(x):
      """시그모이드 함수"""
      return 1 / (1 + np.exp(-x))
      
      
  def prediction(X, theta):
      """로지스틱 회귀 가정 함수"""
      # 지난 과제에서 작성한 코드를 갖고 오세요
      return sigmoid(X @ theta)
      
  
  def gradient_descent(X, theta, y, iterations, alpha):
      """로지스틱 회귀 경사 하강 알고리즘"""
      m = len(X)  # 입력 변수 개수 저장
  
      for _ in range(iterations):
          # 코드를 쓰세요
          error = prediction(X, theta) - y
          theta = theta - alpha / m * (X.T @ error)
              
      return theta
      
      
  # 입력 변수
  hours_studied = np.array([0.2, 0.3, 0.7, 1, 1.3, 1.8, 2, 2.1, 2.2, 3, 4, 4.2, 4, 4.7, 5.0, 5.9])  # 공부 시간 (단위: 100시간)
  gpa_rank = np.array([0.9, 0.95, 0.8, 0.82, 0.7, 0.6, 0.55, 0.67, 0.4, 0.3, 0.2, 0.2, 0.15, 0.18, 0.15, 0.05]) # 학년 내신 (백분률)
  number_of_tries = np.array([1, 2, 2, 2, 4, 2, 2, 2, 3, 3, 3, 3, 2, 4, 1, 2])  # 시험 응시 횟수
  
  # 목표 변수
  passed = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1])  # 시험 통과 여부 (0: 탈락, 1:통과)
  
  # 설계 행렬 X 정의
  X = np.array([
      np.ones(16),
      hours_studied,
      gpa_rank,
      number_of_tries
  ]).T
  
  # 입력 변수 y 정의
  y = passed
  
  theta = [0, 0, 0, 0]  # 파라미터 초기값 설정
  theta = gradient_descent(X, theta, y, 300, 0.1)  # 경사 하강법을 사용해서 최적의 파라미터를 찾는다
  theta
  ```



### 분류가 3개 이상일 때

- 위에서 사용한 가설 함수는 둘 중에 하나로 분류해준다. 따라서 분류가 3개 이상일 때는 가설 함수의 갯수를 늘려서 분류한다
- 예를 들어 3가지 음식(짜장, 짬뽕, 볶음밥)을 구분해야 한다면, 각각을 구분하는 가설 함수(짜장인지 아닌지, 짬뽕인지 아닌지, 볶음밥인지 아닌지에 대한 가설 함수)를 만들어서 학습 시킨다. 그래서 테스트 데이터를 각 가설 함수에 넣어서 가장 높은 확률을 갖는 곳으로 분류한다



### 로지스틱 회귀의 정규 방정식

- 로지스틱 회귀에서는 손실 함수가 아래로 볼록하기는 하다. 하지만 손실 함수를 편미분 했을 때 선형식이 아니므로 단순 행렬 연산만으로는 최소 지점을 찾아낼 수 없다
- 하지만 아래로 볼록이므로 경사 하강법으로는 항상 최적의 세타 값들을 찾아낼 수 있다



### scikit-learn 예시

- ```python
  # ex)
  # 필요한 라이브러리 import
  from sklearn import datasets
  from sklearn.model_selection import train_test_split
  from sklearn.linear_model import LogisticRegression
  
  import pandas as pd  
  
  wine_data = datasets.load_wine()
  """ 데이터 셋을 살펴보는 코드
  print(wine_data.DESCR)
  """
  
  # 입력 변수를 사용하기 편하게 pandas dataframe으로 변환
  X = pd.DataFrame(wine_data.data, columns=wine_data.feature_names)
  
  # 목표 변수를 사용하기 편하게 pandas dataframe으로 변환
  y = pd.DataFrame(wine_data.target, columns=['Y/N'])
  
  # 코드를 쓰세요
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=5)
  y_train = y_train.values.ravel()
  
  logistic_model = LogisticRegression(solver='saga', max_iter=7500)  # sci-kit learn에서 로지스틱 모델을 가지고 온다
  logistic_model.fit(X_train, y_train)
  
  y_test_predict = logistic_model.predict(X_test)
  
  # 테스트 코드
  score = logistic_model.score(X_test, y_test)
  y_test_predict, score
  ```



# 3. 머신 러닝, 더 빠르고 정확하게



# 4. 결정 트리와 앙상블

