# 알고리즘 & 자료구조 관련 정리

## DP

- 조건

  - 최적 부분 구조
    - 부분 문제들의 최적의 답을 이용하면 전체 문제의 최적의 답이 구해지는 구조
  - 중복되는 부분 문제

- 활용

  - 한 번 계산한 결과를 재활용하는 것
  - Memoization
    - 하향식 방법
    - 딕셔너리 cache를 활용하여 재귀로 구현
  - Tabulation
    - 상향식 방법
    - 상향식이기 때문에 문제를 푸는 데 필요없는 부분도 미리 계산하는 경우도 있다. 그래서 Memoization에 비해 약간의 비효율 발생할 수도 있다

- 예시

  - ```python
    def max_profit(stock_list):
        result = stock_list[1] - stock_list[0]
        min_buy_value = stock_list[0]
        
        for i in range(len(stock_list)-1):
            if stock_list[i] < min_buy_value:
                min_buy_value = stock_list[i]
            
            profit = stock_list[i+1] - min_buy_value
            if profit > result:
                result = profit
                
        return result
    
    
    # 테스트
    print(max_profit([7, 1, 5, 3, 6, 4]))
    print(max_profit([7, 6, 4, 3, 1]))
    print(max_profit([11, 13, 9, 13, 20, 14, 19, 12, 19, 13]))
    print(max_profit([12, 4, 11, 18, 17, 19, 1, 19, 14, 13, 7, 15, 10, 1, 3, 6]))
    ```

  - ```python
    def sublist_max(profits):
        result = profits[0]
        cur_max = profits[0]
        
        for i in range(1, len(profits)):
            cur_max = max(cur_max + profits[i], profits[i])
            result = max(result, cur_max)
                    
        return result
    
    
    # 테스트
    print(sublist_max([7, -3, 4, -8]))
    print(sublist_max([-2, -3, 4, -1, -2, 1, 5, -3, -1]))
    ```

  - ```python
    def trapping_rain(buildings):
        width = len(buildings)
        lefts = [0 for _ in range(width)]
        rights = [0 for _ in range(width)]
        
        for i in range(1, width):
            if lefts[i-1] < buildings[i-1]:
                lefts[i] = buildings[i-1]
            else:
                lefts[i] = lefts[i-1]
            
        for i in range(width-2, -1, -1):
            if rights[i+1] < buildings[i+1]:
                rights[i] = buildings[i+1]
            else:
                rights[i] = rights[i+1]
        
        # for left in lefts:
        #     print(left, end=' ')
            
        # print()
        
        # for right in rights:
        #     print(right, end=' ')
        
        # print()
        
        waters = 0    
        for i in range(1, width-1):
            if buildings[i] < lefts[i] and buildings[i] < rights[i]:
                waters += min(lefts[i], rights[i]) - buildings[i]
                
        return waters
    
        
    # 테스트
    print(trapping_rain([3, 0, 0, 2, 0, 4]))
    print(trapping_rain([0, 1, 0, 2, 1, 0, 1, 3, 2, 1, 2, 1]))
    ```



## Greedy Algorithm

- 최적의 답을 구할 수 있는 조건
  - 최적 부분 구조
  - 탐욕적 선택 속성
    - 최적 부분 구조와 다른 점은 시점에 있다. 탐욕적 선택 속성은 지금 당장 목표를 달성하기 위해 탐욕적으로 선택하면, 그 선택이 결과적으로도 최선의 선택일 때를 의미한다



## Sorting

### Merge Sort

```python
## 모범 답안
def merge(list1, list2):
    i = 0
    j = 0

    # 정렬된 항목들을 담을 리스트
    merged_list = []

    # list1과 list2를 돌면서 merged_list에 항목 정렬
    while i < len(list1) and j < len(list2):
        if list1[i] > list2[j]:
            merged_list.append(list2[j])
            j += 1
        else:
            merged_list.append(list1[i])
            i += 1

    # list2에 남은 항목이 있으면 정렬 리스트에 추가
    if i == len(list1):
        merged_list += list2[j:]

    # list1에 남은 항목이 있으면 정렬 리스트에 추가
    elif j == len(list2):
        merged_list += list1[i:]

    return merged_list


def merge_sort(my_list):
    # base case
    if len(my_list) < 2:
        return my_list

    # my_list를 반씩 나눈다(divide)
    left_half = my_list[:len(my_list)//2]    # 왼쪽 반
    right_half = my_list[len(my_list)//2:]   # 오른쪽 반

    # merge_sort 함수를 재귀적으로 호출하여 부분 문제 해결(conquer)하고,
    # merge 함수로 정렬된 두 리스트를 합쳐(combine)준다
    return merge(merge_sort(left_half), merge_sort(right_half))
```

```python
## 내가 짠 것. merge 과정과 쪼개는 과정을 합쳐놔서 가독성 떨어짐
def merge(list1, list2):
    if len(list1) == 0:
        return list2
    elif len(list2) == 0:
        return list1
    
    mid_list1 = len(list1) // 2
    mid_list2 = len(list2) // 2
    
    sorted_list1 = merge(list1[0:mid_list1], list1[mid_list1:])
    sorted_list2 = merge(list2[0:mid_list2], list2[mid_list2:])
    
    result_list = []
    while True:
        if len(sorted_list1) == 0:
            result_list += sorted_list2
            break
        elif len(sorted_list2) == 0:
            result_list += sorted_list1
            break
        
        if sorted_list1[0] <= sorted_list2[0]:
            result_list.append(sorted_list1.pop(0))
        else:
            result_list.append(sorted_list2.pop(0))
            
    return result_list

# 합병 정렬
def merge_sort(my_list):
    mid = len(my_list) // 2
    return merge(my_list[0:mid], my_list[mid:])
```



### Quick Sort

```python
# 두 요소의 위치를 바꿔주는 helper function
def swap_elements(my_list, index1, index2):
    my_list[index1], my_list[index2] = my_list[index2], my_list[index1]
    return


# 퀵 정렬에서 사용되는 partition 함수
def partition(my_list, start, end):
    p = end
    b = start
    i = start
    
    while i != p:
        if my_list[i] > my_list[p]:
            i += 1
            continue
        else:
            swap_elements(my_list, b, i)
            i += 1
            b += 1
            
    swap_elements(my_list, b, p)
    
    return b


def quicksort(my_list, start=0, end=None):
    if end == None:
        end = len(my_list) - 1
    
    if start == end:
        return
    
    p = partition(my_list, start, end)
    quicksort(my_list, start, p-1)
    quicksort(my_list, p, end)
    
    return
```



## 자료구조

- `x in 자료구조` 할 때, list보다 set이 훨씬 빠름
  - 이유?



### 배열(Array)

- list가 여러 타입의 값을 담을 수 있는 이유
  - list의 값이 사실 레퍼런스이기 때문에 실제 데이터가 몇 바이트를 차지하는지 관계 없음
  - C 배열에서는 값에 실제 값들이 들어가야 하기때문에 몇 바이트를 차지해야하는지 선언해주고 시작해야함. 그래서 다른 타입은 담기 어렵기 때문에 C의 배열은 다른 타입을 금지한다
- 동적 배열(ex: list)의 append 연산은 최악의 경우 O(n) 이지만 **분할 상환 분석**을 하면 O(1)이다. 즉 생각보다 아주 비효율적인 것은 아니다라는 것을 의미하는 듯. 내부적으로 처음부터 메모리를 넉넉히 생성해놓기 때문. 따라서 매번 정적 배열 생성해서 값 복사하는 것은 아니니까
  - 반대로 동적 배열의 맨 끝 데이터 삭제 연산도 최악의 경우 O(n) 이지만 **분할 상환 분석**을 하면 O(1)이다. 매번 정적 배열 생성해서 값 복사한 후 메모리 공간을 줄이는 건 아니니까



### 연결 리스트(Linked List)

- `Node` 라는 클래스를 만들고 해당 인스턴스 변수를 `data`, `next` 로 만들어 둔다
- `LinkedList` 라는 클래스를 만들고 `append`, `insert`, `delete` 등 메소드를 만들어 준다. 메소드를 만드는 과정에서 `Node` 클래스의 인스턴스를 생성해서 구현하면 된다
  - 그냥 Node만 가지고 LinkedList 구조를 사용할 수도 있지만 그런 것 보다 `LinkedList` 라는 클래스를 만들고 거기에 `head`, `tail`과 같은 인스턴스 변수를 사용하고, `append`, `delete` 등 LinkedList 구조를 편하게 다룰 수 있는 메소드를 만들면 더 좋으니까



### 해시 테이블(Hash Table)

- 개념
  - 고정된 크기의 배열을 만든다
  - 해시 함수를 이용해서 key를 원하는 범위의 자연수로 바꾼다
    - 해시 함수의 조건
      - 어떤 수를 넣던지 항상 같은 값이 나와야 한다(결정론적이어야 한다)
      - 결과 값이 나오는 확률이 비슷해야 한다
      - 계산이 빨라야 한다(효율적이어야 한다)
  - 해시 함수 결과를 인덱스로 하여, 배열에 key-value 쌍을 저장한다
- Python hash 함수
  - `hash()`: 기본적으로 내장된 hash 함수
  - 파라미터로 받은 값을 특정 범위 안에 있는 정수가 아닌 아무 정수로 바꿔준다
  - 불변 타입 자료형만 파라미터로 받을 수 있다
    - 불린형, 정수형, 소수형, 튜플, 문자열 등
- 해시 테이블 충돌
  - hash 함수를 사용하면 공역이 정의역보다 작기 때문에 다른 파라미터를 넣어도 hash 값이 같을 수 있다. 이를 hash 충돌이라 한다
  - 이를 극복하기 위한 방법 중 하나가 Chaining 이다
    - 배열에 key-value 쌍을 저장할 때 연결 리스트 Node 방식으로 저장하고, 충돌이 날 때마다 이어서 사용하는 것이다. 그러면 같은 hash 값을 가져도 충돌을 극복할 수 있다
  - Open Addressing 방식을 충돌을 극복할 수도 있다
    - 빈 주소에 충돌난 key-value를 저장하는 방식이다
    - Open Address를 찾는 방식은 여러 가지다. 선형 탐사 방식은 충돌난 주소에서부터 한 칸씩 뒤에 있는 인덱스를 넣어보며 빈 주소를 찾는 방식이다. 제곱 탐사 방식은 k^2 (k는 자연수) 뒤에 있는 인덱스를 넣어보며 빈 주소를 찾는 방식이다
    - Open Addressing 방식에서 데이터를 삭제할 때, 그냥 삭제해버리면 안되고 따로 데이터가 삭제되었다는 표시를 해주어야 한다. 왜냐면 해시 충돌이 일어났을 때 탐사 규칙에 따라 탐색을 하게되는데, 중간에 있는 데이터를 그냥 삭제해버리면 그 공간은 원래 데이터가 없는 곳으로 인식된다. 따라서 해당 삭제 공간 이후에 저장된 해시 충돌 데이터를 탐색할 수 없고 데이터가 삭제된 공간에 삭제 표시를 해줘야 계속 모든 데이터를 탐색할 수 있다
- Chaining 방식을 사용하는 해시 테이블의 시간 복잡도
  - 해시 테이블을 사용하는 연산들의 주요 부분 단계
    - 해시 함수 계산, 배열 인덱스 접근, 링크드 리스트 탐색 + (수정, 삽입, 삭제 등의 연산)
    - 이 중 링크드 리스트 탐색 과정이 많은 시간을 소요한다. 최악의 경우 모든 데이터들이 한 배열 인덱스에 저장될 수도 있으니까
  - 최악의 경우(key-value 데이터 들이 모두 해시 충돌이 나는 경우)에는 시간 복잡도가 O(n)
  - 하지만 이런 경우는 드문 경우이다. 동적 배열에서 시간 복잡도를 합리적으로 계산하기 위해 분할 상환 분석 방식을 택했던 것처럼, 해시 테이블에서도 평균 시간 복잡도 방식을 생각해보는 것도 합리적이다
    - hash 함수는 되도록 결과 값이 비슷한 확률로 나오도록 하는 것이 원칙이다. 따라서 key-value 데이터 쌍의 갯수를 n, 배열의 크기를 m 이라 할 때 링크드 리스트의 평균 길이는 n/m 이다
    - 이 경우에 n과 m의 크기를 비슷하게 조절해준다면, 링크드 리스트 탐색에 드는 시간 복잡도는 O(1) 이라고 할 수 있다
    - 따라서 평균 시간 복잡도 방식을 생각해본다면, 해시 테이블의 시간 복잡도는 O(1)이라 생각할 수도 있다. 하지만 역시 최악의 경우는 O(n) 이다
- Open Addressing 방식을 사용하는 해시 테이블의 시간 복잡도
  - Open Addressing 방식도 Chaining 방식처럼 최악의 경우(해시 테이블의 거의 꽉 찼을 경우) 시간 복잡도는 O(n)이다
  - 하지만 해시 테이블이 꽉차 있는 경우는 거의 없고, load factor α = n/m (n은 해시 테이블 안에 들어 있는 데이터 key-value, m은 해시 테이블 배열의 크기)를 고려해서 평균 시간 복잡도로 생각하는 것이 합리적이다
    - 이를 활용하여 계산하면, 탐색에 걸리는 평균 시간 복잡도는 O(1) 이다



### 추상 자료형

- 기능 vs 구현
  - 기능: 연산이 '무엇'을 하는지에 관한 내용
  - 구현: 연산의 기능을 '어떻게'할 지에 관한 내용
- 추상화
  - 구현을 몰라도 기능만 알면 해당 프로그램을 사용할 수 있게 만들어 놓는 것
  - 코드를 재활용하고 협력하는 데 용이함
  - ex) 리스트, 큐, 스택, 딕셔너리, 세트, 트리
- 추상 자료형
  - 자료 구조를 추상화 한 것
  - 데이터를 저장하고 사용할 때 기능만 생각하면 사용할 수 있게 만든 자료형
  - ex) 리스트(추상 자료형) - 동적 배열(자료 구조), 연결 리스트(자료 구조)
  - 추상 자료형을 생각하면 코드의 흐름에 집중할 수 있다
    - 필요한 기능만을 생각해서 추상 자료형을 생각한 후, 구체적으로 어떤 기능을 많이 사용하는 지 등의 기준에 따라 효율적인 자료 구조를 선택해서 이후에 구현하면 되는 것이다
    - ex) 리스트를 동적 배열 형태로 구현하면 접근 연산할 때 효율적이다. 더블리 링크드 리스트로 구현하면 맨 앞 삽입이나 맨 앞 삭제 연산할 때 효율적이다
- python의 deque는 내부적으로 더블리 링크드 리스트로 구현되어 있고, python의 list는 내부적으로 동적 배열로 구현되어 있다
- python dictionary는 내부적으로 해시 테이블을 사용해서 구현되어 있다
- python set는 내부적으로 해시 테이블을 사용해서 구현되어 있다. dictionary 에서는 해시 테이블에 key-value를 저장하지만 set는 key만 저장한다
- 어떤 자료형을 사용할 것인가? list vs set
  - 만약 탐색만을 생각한다면? set 쪽이 훨씬 빠르다. set은 해시 테이블을 사용하므로 O(1)이고 list는 O(n) 이니까
  - 하지만 set은 순서를 구현할 수 없으니 항상 set을 사용할 수는 없을 것이다. 상황에 맞는 자료형을 선택해야 한다. 해당 자료형이 어떤 자료 구조로 만들어져 있는지를 생각해서 효율적인 자료형을 선택해야 한다



### 트리

- 계층적 관계
  - cf) 선형적 자료 구조: 배열, 링크드 리스트
- 트리의 활용
  - 계층적 관계를 가지는 데이터 저장
  - 컴퓨터 과학의 다양한 문제 해결
    - 정렬 문제
    - 압축 문제
  - 다양한 추상 자료형 구현
    - 딕셔너리
    - 우선순위 큐
    - 세트
- 일반적으로 연결 리스트와 비슷하게 `Node`라는 클래스에 `data`라는 인스턴스 변수와 `left_child`, `right_child` 등 관계를 말해주는 레퍼런스를 담는 인스턴스 변수를 생성해서 구현하면 된다
  - 일반적으로 연결 리스트처럼 따로 `LinkedList`같은 클래스를 만들어서 `Node`를 관리하는 형태로는 구현하지 않아도 되는 듯
  - 트리를 구현하는 방법은 여러가지다. 완전 이진 트리 같은 경우에는 완전 이진 트리의 정의 상 각 노드에 인덱스가 부여되어 있는 것이나 마찬가지이기 때문에 리스트로도 구현이 가능하다
    - root 노드의 인덱스를 1이라고 하면, 이 과정에서 인덱스에 규칙이 생긴다. 완전 이진 트리의 자식 노드들은 인덱스\*2, 인덱스\*2+1 이 된다. 따라서 트리를 리스트로 구현해도 계층적 관계를 표현할 수 있다
- 트리 순회
  - 배열에서는 순회할 때 for 문을 많이 썼다. 하지만 트리에는 배열과는 다르게 앞 뒤 순서 관계가 없기 때문에, 여러 스타일의 순회 방식이 있다. 그리고 순회 방식이 정해짐에 따라 배열처럼 앞 뒤 순서를 가지는 선형적 관계를 만들어 쓸 수 있다
    - ex) pre-order, post-order, in-order
  - 트리를 순회할 때는 재귀를 많이 쓴다
  - pre-order
    - **현재 노드 데이터 처리(ex: 데이터 출력)** -> 왼쪽 부분 트리 순회 -> 오른쪽 부분 트리 순회
  - post-order
    - 왼쪽 부분 트리 순회 -> 오른쪽 부분 트리 순회 -> **현재 노드 데이터 처리**
  - in-order
    - 왼쪽 부분 트리 순회 -> **현재 노드 데이터 처리** -> 오른쪽 부분 트리 순회



### 힙

- 두 개의 조건을 만족하는 트리
  - 형태 속성: 완전 이진 트리
  - 힙 속성: 부모 노드의 데이터 >= 자식 노드들의 데이터
  
- **정렬**과 **우선 순위 큐**를 구현할 때 쓰임

- 힙은 완전 이진 트리이므로 배열로 구현할 수 있다(위에 트리 메모 부분에 적어놓은 내용)

- heapify
  - 트리 구조에서 힙 속성을 갖도록 노드를 재배치하는 알고리즘
  - 최악의 경우 재배치하려고 하는목표 노드가 맨 위에서 트리 높이 만큼 내려올테니까 시간복잡도는 O(log n)
  - 완전 이진 트리에서 힙을 만드려면 힙 속성을 갖추게 하면 된다. 맨 마지막 노드들에서부터 차례로 heapify 알고리즘을 적용하면 힙이 완성된다. 이때 한 노드의 heapify O(log n)이므로 시간복잡도는 O(nlog n)이다
  
- 힙 정렬

  - 힙의 속성을 생각해본다면, root 노드에는 가장 큰 데이터가 들어 있다
    - 하지만 root 노드의 자식 간에는 크기에 따른 순서를 두지 않았으므로 힙을 그냥 순회하는 것으로 정렬이 되었다고 볼 수 없다
  - 힙 정렬 방법
    - 힙을 만든다
    - root와 마지막 노드 swap 후 새로운 마지막 노드는 없는 노드로 본다
    - 새로운 root 노드가 힙 속성을 만족하도록 heapify 알고리즘을 적용한다
    - 마지막 노드를 없는 노드로 보기 때문에, 위 과정이 수행되는 트리의 노드 수는 한 개씩 줄어든다. 위 과정을 모든 인덱스에 반복한다
  - 만약 힙 정렬을 내림차순으로 하고 싶다면, 힙 속성을 거꾸로 잡으면 된다(부모 노드의 데이터 <= 자식 노드의 데이터)

- ```python
  ## 힙 정렬 예시
  def swap(tree, index_1, index_2):
      temp = tree[index_1]
      tree[index_1] = tree[index_2]
      tree[index_2] = temp
  
  
  def heapify(tree, index, tree_size):
      """heapify 함수"""
  
      # 왼쪽 자식 노드의 인덱스와 오른쪽 자식 노드의 인덱스를 계산
      left_child_index = 2 * index
      right_child_index = 2 * index + 1
  
      largest = index  # 일단 부모 노드의 값이 가장 크다고 설정
  
      # 왼쪽 자식 노드의 값과 비교
      if 0 < left_child_index < tree_size and tree[largest] < tree[left_child_index]:
          largest = left_child_index
  
      # 오른쪽 자식 노드의 값과 비교
      if 0 < right_child_index < tree_size and tree[largest] < tree[right_child_index]:
          largest = right_child_index
      
      if largest != index: # 부모 노드의 값이 자식 노드의 값보다 작으면
          swap(tree, index, largest)  # 부모 노드와 최댓값을 가진 자식 노드의 위치를 바꿔준다
          heapify(tree, largest, tree_size)  # 자리가 바뀌어 자식 노드가 된 기존의 부모 노드를대상으로 또 heapify 함수를 호출한다
  
  def heapsort(tree):
      """힙 정렬 함수"""
      tree_size = len(tree)
  
      for i in range(tree_size, 0, -1):
          heapify(tree, i, tree_size)
      
      for i in range(tree_size-1, 0, -1):
          swap(tree, i, 1)
          heapify(tree, 1, i)
  
      return
  
  
  # 실행 코드
  data_to_sort = [None, 6, 1, 4, 7, 10, 3, 8, 5, 1, 5, 7, 4, 2, 1]
  heapsort(data_to_sort)
  print(data_to_sort)
  ```

- 우선 순위 큐

  - 저장한 데이터를 우선 순위로 꺼내 쓰는 큐
  - 힙으로 구현 가능
    - 데이터 삽입
      - 새로운 데이터를 tree 마지막 노드로 추가
      - 마지막 노드가 힙 속성을 유지할 수 있도록 해야 함
        - 이 때 다른 노드들은 이미 힙 속성을 갖고 있으므로 모든 노드에 heapify를 할 필요는 없고 부모 노드를 타고 올라가면서 힙 속성을 유지할 수 있도록 reverse_heapify를 새로 만들어서 힙 속성 유지하도록 하면 됨
    - 데이터 추출
      - root 노드와 마지막 노드 swap
      - 새로운 마지막노드 데이터를 변수에 저장한 후 삭제
      - root 노드에 heapify를 적용해서 망가진 힙 속성을 수정
      - 변수에 저장한 최고 우선 순위 데이터 반환
  - 우선 순위 큐를 힙으로 구현할 때, 우선 순위의 기준은 힙 속성을 어떻게 정의하느냐에 달려 있다
    - ex) 맨 처음에 힙 속성 배운 것처럼 "부모 노드의 데이터 >= 자식 노드들의 데이터"로 해도 되고, 특정 기준을 정해서 해도 무방할 듯
  - 우선 순위 큐는 동적 배열과 더블리 링크드 리스트을 항상 정렬 시켜 놓는 방법으로 구현할 수도 있다
    - 이 때, 데이터 추출은 동적 배열과 더블리 링크드 리스트가 O(1)로 유리하다. 힙은 데이터 추출하고 난 후에 계속 힙을 유지해야 하므로 O(log n)이니까
    - 하지만 데이터 삽입에서 동적 배열과 더블리 링크드 리스트는 O(n) 이다. 동적 배열은 최악의 경우 싹 밀어서 다시 저장해야 하고, 더블리 링크드 리스트는 최악의 경우 맨 앞부터 순서대로 다 탐색해야 하니까. 힙은 삽입의 경우에도 O(log n) 이다
    - 따라서 데이터를 삽입할 일이 많으면 힙, 데이터를 추출할 일이 많으면 동적 배열 혹은 더블리 링크드 리스트로 구현하는 게 좋다

- ```python
  ## 힙으로 구현한 우선 순위 큐 예시
  ## heapify_code.py
  def swap(tree, index_1, index_2):
      """완전 이진 트리의 노드 index_1과 노드 index_2의 위치를 바꿔준다"""
      temp = tree[index_1]
      tree[index_1] = tree[index_2]
      tree[index_2] = temp
  
  
  def heapify(tree, index, tree_size):
      """heapify 함수"""
  
      # 왼쪽 자식 노드의 인덱스와 오른쪽 자식 노드의 인덱스를 계산
      left_child_index = 2 * index
      right_child_index = 2 * index + 1
  
      largest = index  # 일단 부모 노드의 값이 가장 크다고 설정
  
      # 왼쪽 자식 노드의 값과 비교
      if 0 < left_child_index < tree_size and tree[largest] < tree[left_child_index]:
          largest = left_child_index
  
      # 오른쪽 자식 노드의 값과 비교
      if 0 < right_child_index < tree_size and tree[largest] < tree[right_child_index]:
          largest = right_child_index
      
      if largest != index: # 부모 노드의 값이 자식 노드의 값보다 작으면
          swap(tree, index, largest)  # 부모 노드와 최댓값을 가진 자식 노드의 위치를 바꿔준다
          heapify(tree, largest, tree_size)  # 자리가 바뀌어 자식 노드가 된 기존의 부모 노드를대상으로 또 heapify 함수를 호출한다
  
  
  def reverse_heapify(tree, index):
      """삽입된 노드를 힙 속성을 지키는 위치로 이동시키는 함수"""
      parent_index = index // 2  # 삽입된 노드의 부모 노드의 인덱스 계산
  
      # 부모 노드가 존재하고, 부모 노드의 값이 삽입된 노드의 값보다 작을 때
      if 0 < parent_index < len(tree) and tree[index] > tree[parent_index]:
          swap(tree, index, parent_index)  # 부모 노드와 삽입된 노드의 위치 교환
          reverse_heapify(tree, parent_index)  # 삽입된 노드를 대상으로 다시 reverse_heapify 호출
  
  
  ## main.py
  from heapify_code import *
  
  class PriorityQueue:
      """힙으로 구현한 우선순위 큐"""
      def __init__(self):
          self.heap = [None]  # 파이썬 리스트로 구현한 힙
  
      def insert(self, data):
          """삽입 메소드"""
          self.heap.append(data)  # 힙의 마지막에 데이터 추가
          reverse_heapify(self.heap, len(self.heap)-1) # 삽입된 노드(추가된 데이터)의 위치를 재배치
  
      def extract_max(self):
          """최우선순위 데이터 추출 메소드"""
          # 코드를 쓰세요
          swap(self.heap, 1, len(self.heap)-1)
          result = self.heap.pop()
          heapify(self.heap, 1, len(self.heap))
          
          return result
  
      def __str__(self):
          return str(self.heap)
  
  # 출력 코드
  priority_queue = PriorityQueue()
  
  priority_queue.insert(6)
  priority_queue.insert(9)
  priority_queue.insert(1)
  priority_queue.insert(3)
  priority_queue.insert(10)
  priority_queue.insert(11)
  priority_queue.insert(13)
  
  print(priority_queue.extract_max())
  print(priority_queue.extract_max())
  print(priority_queue.extract_max())
  print(priority_queue.extract_max())
  print(priority_queue.extract_max())
  print(priority_queue.extract_max())
  print(priority_queue.extract_max())
  ```

- 