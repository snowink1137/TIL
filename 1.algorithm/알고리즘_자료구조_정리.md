# 알고리즘 & 자료구조 관련 정리

[TOC]



## DP

- 조건

  - 최적 부분 구조
    - 부분 문제들의 최적의 답을 이용하면 전체 문제의 최적의 답이 구해지는 구조
  - 중복되는 부분 문제

- 활용

  - 한 번 계산한 결과를 재활용하는 것
  - Memoization
    - 하향식 방법
    - 딕셔너리 cache를 활용하여 재귀로 구현
  - Tabulation
    - 상향식 방법
    - 상향식이기 때문에 문제를 푸는 데 필요없는 부분도 미리 계산하는 경우도 있다. 그래서 Memoization에 비해 약간의 비효율 발생할 수도 있다

- 예시

  - ```python
    def max_profit(stock_list):
        result = stock_list[1] - stock_list[0]
        min_buy_value = stock_list[0]
        
        for i in range(len(stock_list)-1):
            if stock_list[i] < min_buy_value:
                min_buy_value = stock_list[i]
            
            profit = stock_list[i+1] - min_buy_value
            if profit > result:
                result = profit
                
        return result
    
    
    # 테스트
    print(max_profit([7, 1, 5, 3, 6, 4]))
    print(max_profit([7, 6, 4, 3, 1]))
    print(max_profit([11, 13, 9, 13, 20, 14, 19, 12, 19, 13]))
    print(max_profit([12, 4, 11, 18, 17, 19, 1, 19, 14, 13, 7, 15, 10, 1, 3, 6]))
    ```

  - ```python
    def sublist_max(profits):
        result = profits[0]
        cur_max = profits[0]
        
        for i in range(1, len(profits)):
            cur_max = max(cur_max + profits[i], profits[i])
            result = max(result, cur_max)
                    
        return result
    
    
    # 테스트
    print(sublist_max([7, -3, 4, -8]))
    print(sublist_max([-2, -3, 4, -1, -2, 1, 5, -3, -1]))
    ```

  - ```python
    def trapping_rain(buildings):
        width = len(buildings)
        lefts = [0 for _ in range(width)]
        rights = [0 for _ in range(width)]
        
        for i in range(1, width):
            if lefts[i-1] < buildings[i-1]:
                lefts[i] = buildings[i-1]
            else:
                lefts[i] = lefts[i-1]
            
        for i in range(width-2, -1, -1):
            if rights[i+1] < buildings[i+1]:
                rights[i] = buildings[i+1]
            else:
                rights[i] = rights[i+1]
        
        # for left in lefts:
        #     print(left, end=' ')
            
        # print()
        
        # for right in rights:
        #     print(right, end=' ')
        
        # print()
        
        waters = 0    
        for i in range(1, width-1):
            if buildings[i] < lefts[i] and buildings[i] < rights[i]:
                waters += min(lefts[i], rights[i]) - buildings[i]
                
        return waters
    
        
    # 테스트
    print(trapping_rain([3, 0, 0, 2, 0, 4]))
    print(trapping_rain([0, 1, 0, 2, 1, 0, 1, 3, 2, 1, 2, 1]))
    ```



## Greedy Algorithm

- 최적의 답을 구할 수 있는 조건
  - 최적 부분 구조
  - 탐욕적 선택 속성
    - 최적 부분 구조와 다른 점은 시점에 있다. 탐욕적 선택 속성은 지금 당장 목표를 달성하기 위해 탐욕적으로 선택하면, 그 선택이 결과적으로도 최선의 선택일 때를 의미한다



## Sorting

### Merge Sort

```python
## 모범 답안
def merge(list1, list2):
    i = 0
    j = 0

    # 정렬된 항목들을 담을 리스트
    merged_list = []

    # list1과 list2를 돌면서 merged_list에 항목 정렬
    while i < len(list1) and j < len(list2):
        if list1[i] > list2[j]:
            merged_list.append(list2[j])
            j += 1
        else:
            merged_list.append(list1[i])
            i += 1

    # list2에 남은 항목이 있으면 정렬 리스트에 추가
    if i == len(list1):
        merged_list += list2[j:]

    # list1에 남은 항목이 있으면 정렬 리스트에 추가
    elif j == len(list2):
        merged_list += list1[i:]

    return merged_list


def merge_sort(my_list):
    # base case
    if len(my_list) < 2:
        return my_list

    # my_list를 반씩 나눈다(divide)
    left_half = my_list[:len(my_list)//2]    # 왼쪽 반
    right_half = my_list[len(my_list)//2:]   # 오른쪽 반

    # merge_sort 함수를 재귀적으로 호출하여 부분 문제 해결(conquer)하고,
    # merge 함수로 정렬된 두 리스트를 합쳐(combine)준다
    return merge(merge_sort(left_half), merge_sort(right_half))
```

```python
## 내가 짠 것. merge 과정과 쪼개는 과정을 합쳐놔서 가독성 떨어짐
def merge(list1, list2):
    if len(list1) == 0:
        return list2
    elif len(list2) == 0:
        return list1
    
    mid_list1 = len(list1) // 2
    mid_list2 = len(list2) // 2
    
    sorted_list1 = merge(list1[0:mid_list1], list1[mid_list1:])
    sorted_list2 = merge(list2[0:mid_list2], list2[mid_list2:])
    
    result_list = []
    while True:
        if len(sorted_list1) == 0:
            result_list += sorted_list2
            break
        elif len(sorted_list2) == 0:
            result_list += sorted_list1
            break
        
        if sorted_list1[0] <= sorted_list2[0]:
            result_list.append(sorted_list1.pop(0))
        else:
            result_list.append(sorted_list2.pop(0))
            
    return result_list

# 합병 정렬
def merge_sort(my_list):
    mid = len(my_list) // 2
    return merge(my_list[0:mid], my_list[mid:])
```



### Quick Sort

```python
# 두 요소의 위치를 바꿔주는 helper function
def swap_elements(my_list, index1, index2):
    my_list[index1], my_list[index2] = my_list[index2], my_list[index1]
    return


# 퀵 정렬에서 사용되는 partition 함수
def partition(my_list, start, end):
    p = end
    b = start
    i = start
    
    while i != p:
        if my_list[i] > my_list[p]:
            i += 1
            continue
        else:
            swap_elements(my_list, b, i)
            i += 1
            b += 1
            
    swap_elements(my_list, b, p)
    
    return b


def quicksort(my_list, start=0, end=None):
    if end == None:
        end = len(my_list) - 1
    
    if start == end:
        return
    
    p = partition(my_list, start, end)
    quicksort(my_list, start, p-1)
    quicksort(my_list, p, end)
    
    return
```



## 자료구조

- `x in 자료구조` 할 때, list보다 set이 훨씬 빠름
  - 이유?



### 배열(Array)

- list가 여러 타입의 값을 담을 수 있는 이유
  - list의 값이 사실 레퍼런스이기 때문에 실제 데이터가 몇 바이트를 차지하는지 관계 없음
  - C 배열에서는 값에 실제 값들이 들어가야 하기때문에 몇 바이트를 차지해야하는지 선언해주고 시작해야함. 그래서 다른 타입은 담기 어렵기 때문에 C의 배열은 다른 타입을 금지한다
- 동적 배열(ex: list)의 append 연산은 최악의 경우 O(n) 이지만 **분할 상환 분석**을 하면 O(1)이다. 즉 생각보다 아주 비효율적인 것은 아니다라는 것을 의미하는 듯. 내부적으로 처음부터 메모리를 넉넉히 생성해놓기 때문. 따라서 매번 정적 배열 생성해서 값 복사하는 것은 아니니까
  - 반대로 동적 배열의 맨 끝 데이터 삭제 연산도 최악의 경우 O(n) 이지만 **분할 상환 분석**을 하면 O(1)이다. 매번 정적 배열 생성해서 값 복사한 후 메모리 공간을 줄이는 건 아니니까



### 연결 리스트(Linked List)

- `Node` 라는 클래스를 만들고 해당 인스턴스 변수를 `data`, `next` 로 만들어 둔다
- `LinkedList` 라는 클래스를 만들고 `append`, `insert`, `delete` 등 메소드를 만들어 준다. 메소드를 만드는 과정에서 `Node` 클래스의 인스턴스를 생성해서 구현하면 된다
  - 그냥 Node만 가지고 LinkedList 구조를 사용할 수도 있지만 그런 것 보다 `LinkedList` 라는 클래스를 만들고 거기에 `head`, `tail`과 같은 인스턴스 변수를 사용하고, `append`, `delete` 등 LinkedList 구조를 편하게 다룰 수 있는 메소드를 만들면 더 좋으니까



### 해시 테이블(Hash Table)

- 개념
  - 고정된 크기의 배열을 만든다
  - 해시 함수를 이용해서 key를 원하는 범위의 자연수로 바꾼다
    - 해시 함수의 조건
      - 어떤 수를 넣던지 항상 같은 값이 나와야 한다(결정론적이어야 한다)
      - 결과 값이 나오는 확률이 비슷해야 한다
      - 계산이 빨라야 한다(효율적이어야 한다)
  - 해시 함수 결과를 인덱스로 하여, 배열에 key-value 쌍을 저장한다
- Python hash 함수
  - `hash()`: 기본적으로 내장된 hash 함수
  - 파라미터로 받은 값을 특정 범위 안에 있는 정수가 아닌 아무 정수로 바꿔준다
  - 불변 타입 자료형만 파라미터로 받을 수 있다
    - 불린형, 정수형, 소수형, 튜플, 문자열 등
- 해시 테이블 충돌
  - hash 함수를 사용하면 공역이 정의역보다 작기 때문에 다른 파라미터를 넣어도 hash 값이 같을 수 있다. 이를 hash 충돌이라 한다
  - 이를 극복하기 위한 방법 중 하나가 Chaining 이다
    - 배열에 key-value 쌍을 저장할 때 연결 리스트 Node 방식으로 저장하고, 충돌이 날 때마다 이어서 사용하는 것이다. 그러면 같은 hash 값을 가져도 충돌을 극복할 수 있다
  - Open Addressing 방식을 충돌을 극복할 수도 있다
    - 빈 주소에 충돌난 key-value를 저장하는 방식이다
    - Open Address를 찾는 방식은 여러 가지다. 선형 탐사 방식은 충돌난 주소에서부터 한 칸씩 뒤에 있는 인덱스를 넣어보며 빈 주소를 찾는 방식이다. 제곱 탐사 방식은 k^2 (k는 자연수) 뒤에 있는 인덱스를 넣어보며 빈 주소를 찾는 방식이다
    - Open Addressing 방식에서 데이터를 삭제할 때, 그냥 삭제해버리면 안되고 따로 데이터가 삭제되었다는 표시를 해주어야 한다. 왜냐면 해시 충돌이 일어났을 때 탐사 규칙에 따라 탐색을 하게되는데, 중간에 있는 데이터를 그냥 삭제해버리면 그 공간은 원래 데이터가 없는 곳으로 인식된다. 따라서 해당 삭제 공간 이후에 저장된 해시 충돌 데이터를 탐색할 수 없고 데이터가 삭제된 공간에 삭제 표시를 해줘야 계속 모든 데이터를 탐색할 수 있다
- Chaining 방식을 사용하는 해시 테이블의 시간 복잡도
  - 해시 테이블을 사용하는 연산들의 주요 부분 단계
    - 해시 함수 계산, 배열 인덱스 접근, 링크드 리스트 탐색 + (수정, 삽입, 삭제 등의 연산)
    - 이 중 링크드 리스트 탐색 과정이 많은 시간을 소요한다. 최악의 경우 모든 데이터들이 한 배열 인덱스에 저장될 수도 있으니까
  - 최악의 경우(key-value 데이터 들이 모두 해시 충돌이 나는 경우)에는 시간 복잡도가 O(n)
  - 하지만 이런 경우는 드문 경우이다. 동적 배열에서 시간 복잡도를 합리적으로 계산하기 위해 분할 상환 분석 방식을 택했던 것처럼, 해시 테이블에서도 평균 시간 복잡도 방식을 생각해보는 것도 합리적이다
    - hash 함수는 되도록 결과 값이 비슷한 확률로 나오도록 하는 것이 원칙이다. 따라서 key-value 데이터 쌍의 갯수를 n, 배열의 크기를 m 이라 할 때 링크드 리스트의 평균 길이는 n/m 이다
    - 이 경우에 n과 m의 크기를 비슷하게 조절해준다면, 링크드 리스트 탐색에 드는 시간 복잡도는 O(1) 이라고 할 수 있다
    - 따라서 평균 시간 복잡도 방식을 생각해본다면, 해시 테이블의 시간 복잡도는 O(1)이라 생각할 수도 있다. 하지만 역시 최악의 경우는 O(n) 이다
- Open Addressing 방식을 사용하는 해시 테이블의 시간 복잡도
  - Open Addressing 방식도 Chaining 방식처럼 최악의 경우(해시 테이블의 거의 꽉 찼을 경우) 시간 복잡도는 O(n)이다
  - 하지만 해시 테이블이 꽉차 있는 경우는 거의 없고, load factor α = n/m (n은 해시 테이블 안에 들어 있는 데이터 key-value, m은 해시 테이블 배열의 크기)를 고려해서 평균 시간 복잡도로 생각하는 것이 합리적이다
    - 이를 활용하여 계산하면, 탐색에 걸리는 평균 시간 복잡도는 O(1) 이다



### 추상 자료형

- 기능 vs 구현
  - 기능: 연산이 '무엇'을 하는지에 관한 내용
  - 구현: 연산의 기능을 '어떻게'할 지에 관한 내용
- 추상화
  - 구현을 몰라도 기능만 알면 해당 프로그램을 사용할 수 있게 만들어 놓는 것
  - 코드를 재활용하고 협력하는 데 용이함
  - ex) 리스트, 큐, 스택, 딕셔너리, 세트, 트리
- 추상 자료형
  - 자료 구조를 추상화 한 것
  - 데이터를 저장하고 사용할 때 기능만 생각하면 사용할 수 있게 만든 자료형
  - ex) 리스트(추상 자료형) - 동적 배열(자료 구조), 연결 리스트(자료 구조)
  - 추상 자료형을 생각하면 코드의 흐름에 집중할 수 있다
    - 필요한 기능만을 생각해서 추상 자료형을 생각한 후, 구체적으로 어떤 기능을 많이 사용하는 지 등의 기준에 따라 효율적인 자료 구조를 선택해서 이후에 구현하면 되는 것이다
    - ex) 리스트를 동적 배열 형태로 구현하면 접근 연산할 때 효율적이다. 더블리 링크드 리스트로 구현하면 맨 앞 삽입이나 맨 앞 삭제 연산할 때 효율적이다
- python의 deque는 내부적으로 더블리 링크드 리스트로 구현되어 있고, python의 list는 내부적으로 동적 배열로 구현되어 있다
- python dictionary는 내부적으로 해시 테이블을 사용해서 구현되어 있다
- python set는 내부적으로 해시 테이블을 사용해서 구현되어 있다. dictionary 에서는 해시 테이블에 key-value를 저장하지만 set는 key만 저장한다
- 어떤 자료형을 사용할 것인가? list vs set
  - 만약 탐색만을 생각한다면? set 쪽이 훨씬 빠르다. set은 해시 테이블을 사용하므로 O(1)이고 list는 O(n) 이니까
  - 하지만 set은 순서를 구현할 수 없으니 항상 set을 사용할 수는 없을 것이다. 상황에 맞는 자료형을 선택해야 한다. 해당 자료형이 어떤 자료 구조로 만들어져 있는지를 생각해서 효율적인 자료형을 선택해야 한다



### 트리

- 계층적 관계
  - cf) 선형적 자료 구조: 배열, 링크드 리스트
- 트리의 활용
  - 계층적 관계를 가지는 데이터 저장
  - 컴퓨터 과학의 다양한 문제 해결
    - 정렬 문제
    - 압축 문제
  - 다양한 추상 자료형 구현
    - 딕셔너리
    - 우선순위 큐
    - 세트
- 일반적으로 연결 리스트와 비슷하게 `Node`라는 클래스에 `data`라는 인스턴스 변수와 `left_child`, `right_child` 등 관계를 말해주는 레퍼런스를 담는 인스턴스 변수를 생성해서 구현하면 된다
  - 일반적으로 연결 리스트처럼 따로 `LinkedList`같은 클래스를 만들어서 `Node`를 관리하는 형태로는 구현하지 않아도 되는 듯
  - 트리를 구현하는 방법은 여러가지다. 완전 이진 트리 같은 경우에는 완전 이진 트리의 정의 상 각 노드에 인덱스가 부여되어 있는 것이나 마찬가지이기 때문에 리스트로도 구현이 가능하다
    - root 노드의 인덱스를 1이라고 하면, 이 과정에서 인덱스에 규칙이 생긴다. 완전 이진 트리의 자식 노드들은 인덱스\*2, 인덱스\*2+1 이 된다. 따라서 트리를 리스트로 구현해도 계층적 관계를 표현할 수 있다
- 트리 순회
  - 배열에서는 순회할 때 for 문을 많이 썼다. 하지만 트리에는 배열과는 다르게 앞 뒤 순서 관계가 없기 때문에, 여러 스타일의 순회 방식이 있다. 그리고 순회 방식이 정해짐에 따라 배열처럼 앞 뒤 순서를 가지는 선형적 관계를 만들어 쓸 수 있다
    - ex) pre-order, post-order, in-order
  - 트리를 순회할 때는 재귀를 많이 쓴다
  - pre-order
    - **현재 노드 데이터 처리(ex: 데이터 출력)** -> 왼쪽 부분 트리 순회 -> 오른쪽 부분 트리 순회
  - post-order
    - 왼쪽 부분 트리 순회 -> 오른쪽 부분 트리 순회 -> **현재 노드 데이터 처리**
  - in-order
    - 왼쪽 부분 트리 순회 -> **현재 노드 데이터 처리** -> 오른쪽 부분 트리 순회



### 힙

- 두 개의 조건을 만족하는 트리
  - 형태 속성: 완전 이진 트리
  - 힙 속성: 부모 노드의 데이터 >= 자식 노드들의 데이터
  
- **정렬**과 **우선 순위 큐**를 구현할 때 쓰임

- 힙은 완전 이진 트리이므로 배열로 구현할 수 있다(위에 트리 메모 부분에 적어놓은 내용)

- heapify
  - 트리 구조에서 힙 속성을 갖도록 노드를 재배치하는 알고리즘
  - 최악의 경우 재배치하려고 하는목표 노드가 맨 위에서 트리 높이 만큼 내려올테니까 시간복잡도는 O(log n)
  - 완전 이진 트리에서 힙을 만드려면 힙 속성을 갖추게 하면 된다. 맨 마지막 노드들에서부터 차례로 heapify 알고리즘을 적용하면 힙이 완성된다. 이때 한 노드의 heapify O(log n)이므로 시간복잡도는 O(nlog n)이다
  
- 힙 정렬

  - 힙의 속성을 생각해본다면, root 노드에는 가장 큰 데이터가 들어 있다
    - 하지만 root 노드의 자식 간에는 크기에 따른 순서를 두지 않았으므로 힙을 그냥 순회하는 것으로 정렬이 되었다고 볼 수 없다
  - 힙 정렬 방법
    - 힙을 만든다
    - root와 마지막 노드 swap 후 새로운 마지막 노드는 없는 노드로 본다
    - 새로운 root 노드가 힙 속성을 만족하도록 heapify 알고리즘을 적용한다
    - 마지막 노드를 없는 노드로 보기 때문에, 위 과정이 수행되는 트리의 노드 수는 한 개씩 줄어든다. 위 과정을 모든 인덱스에 반복한다
  - 만약 힙 정렬을 내림차순으로 하고 싶다면, 힙 속성을 거꾸로 잡으면 된다(부모 노드의 데이터 <= 자식 노드의 데이터)

- ```python
  ## 힙 정렬 예시
  def swap(tree, index_1, index_2):
      temp = tree[index_1]
      tree[index_1] = tree[index_2]
      tree[index_2] = temp
  
  
  def heapify(tree, index, tree_size):
      """heapify 함수"""
  
      # 왼쪽 자식 노드의 인덱스와 오른쪽 자식 노드의 인덱스를 계산
      left_child_index = 2 * index
      right_child_index = 2 * index + 1
  
      largest = index  # 일단 부모 노드의 값이 가장 크다고 설정
  
      # 왼쪽 자식 노드의 값과 비교
      if 0 < left_child_index < tree_size and tree[largest] < tree[left_child_index]:
          largest = left_child_index
  
      # 오른쪽 자식 노드의 값과 비교
      if 0 < right_child_index < tree_size and tree[largest] < tree[right_child_index]:
          largest = right_child_index
      
      if largest != index: # 부모 노드의 값이 자식 노드의 값보다 작으면
          swap(tree, index, largest)  # 부모 노드와 최댓값을 가진 자식 노드의 위치를 바꿔준다
          heapify(tree, largest, tree_size)  # 자리가 바뀌어 자식 노드가 된 기존의 부모 노드를대상으로 또 heapify 함수를 호출한다
  
  def heapsort(tree):
      """힙 정렬 함수"""
      tree_size = len(tree)
  
      for i in range(tree_size, 0, -1):
          heapify(tree, i, tree_size)
      
      for i in range(tree_size-1, 0, -1):
          swap(tree, i, 1)
          heapify(tree, 1, i)
  
      return
  
  
  # 실행 코드
  data_to_sort = [None, 6, 1, 4, 7, 10, 3, 8, 5, 1, 5, 7, 4, 2, 1]
  heapsort(data_to_sort)
  print(data_to_sort)
  ```




#### 우선 순위 큐

- 저장한 데이터를 우선 순위로 꺼내 쓰는 큐
- 힙으로 구현 가능
  - 데이터 삽입
    - 새로운 데이터를 tree 마지막 노드로 추가
    - 마지막 노드가 힙 속성을 유지할 수 있도록 해야 함
      - 이 때 다른 노드들은 이미 힙 속성을 갖고 있으므로 모든 노드에 heapify를 할 필요는 없고 부모 노드를 타고 올라가면서 힙 속성을 유지할 수 있도록 reverse_heapify를 새로 만들어서 힙 속성 유지하도록 하면 됨
  - 데이터 추출
    - root 노드와 마지막 노드 swap
    - 새로운 마지막노드 데이터를 변수에 저장한 후 삭제
    - root 노드에 heapify를 적용해서 망가진 힙 속성을 수정
    - 변수에 저장한 최고 우선 순위 데이터 반환

- 우선 순위 큐를 힙으로 구현할 때, 우선 순위의 기준은 힙 속성을 어떻게 정의하느냐에 달려 있다

  - ex) 맨 처음에 힙 속성 배운 것처럼 "부모 노드의 데이터 >= 자식 노드들의 데이터"로 해도 되고, 특정 기준을 정해서 해도 무방할 듯

- 우선 순위 큐는 동적 배열과 더블리 링크드 리스트을 항상 정렬 시켜 놓는 방법으로 구현할 수도 있다
  - 이 때, 데이터 추출은 동적 배열과 더블리 링크드 리스트가 O(1)로 유리하다. 힙은 데이터 추출하고 난 후에 계속 힙을 유지해야 하므로 O(log n)이니까
  - 하지만 데이터 삽입에서 동적 배열과 더블리 링크드 리스트는 O(n) 이다. 동적 배열은 최악의 경우 싹 밀어서 다시 저장해야 하고, 더블리 링크드 리스트는 최악의 경우 맨 앞부터 순서대로 다 탐색해야 하니까. 힙은 삽입의 경우에도 O(log n) 이다
  - 따라서 데이터를 삽입할 일이 많으면 힙, 데이터를 추출할 일이 많으면 동적 배열 혹은 더블리 링크드 리스트로 구현하는 게 좋다

- ```python
  ## 힙으로 구현한 우선 순위 큐 예시
  ## heapify_code.py
  def swap(tree, index_1, index_2):
      """완전 이진 트리의 노드 index_1과 노드 index_2의 위치를 바꿔준다"""
      temp = tree[index_1]
      tree[index_1] = tree[index_2]
      tree[index_2] = temp
  
  
  def heapify(tree, index, tree_size):
      """heapify 함수"""
  
      # 왼쪽 자식 노드의 인덱스와 오른쪽 자식 노드의 인덱스를 계산
      left_child_index = 2 * index
      right_child_index = 2 * index + 1
  
      largest = index  # 일단 부모 노드의 값이 가장 크다고 설정
  
      # 왼쪽 자식 노드의 값과 비교
      if 0 < left_child_index < tree_size and tree[largest] < tree[left_child_index]:
          largest = left_child_index
  
      # 오른쪽 자식 노드의 값과 비교
      if 0 < right_child_index < tree_size and tree[largest] < tree[right_child_index]:
          largest = right_child_index
      
      if largest != index: # 부모 노드의 값이 자식 노드의 값보다 작으면
          swap(tree, index, largest)  # 부모 노드와 최댓값을 가진 자식 노드의 위치를 바꿔준다
          heapify(tree, largest, tree_size)  # 자리가 바뀌어 자식 노드가 된 기존의 부모 노드를대상으로 또 heapify 함수를 호출한다
  
  
  def reverse_heapify(tree, index):
      """삽입된 노드를 힙 속성을 지키는 위치로 이동시키는 함수"""
      parent_index = index // 2  # 삽입된 노드의 부모 노드의 인덱스 계산
  
      # 부모 노드가 존재하고, 부모 노드의 값이 삽입된 노드의 값보다 작을 때
      if 0 < parent_index < len(tree) and tree[index] > tree[parent_index]:
          swap(tree, index, parent_index)  # 부모 노드와 삽입된 노드의 위치 교환
          reverse_heapify(tree, parent_index)  # 삽입된 노드를 대상으로 다시 reverse_heapify 호출
  
  
  ## main.py
  from heapify_code import *
  
  class PriorityQueue:
      """힙으로 구현한 우선순위 큐"""
      def __init__(self):
          self.heap = [None]  # 파이썬 리스트로 구현한 힙
  
      def insert(self, data):
          """삽입 메소드"""
          self.heap.append(data)  # 힙의 마지막에 데이터 추가
          reverse_heapify(self.heap, len(self.heap)-1) # 삽입된 노드(추가된 데이터)의 위치를 재배치
  
      def extract_max(self):
          """최우선순위 데이터 추출 메소드"""
          # 코드를 쓰세요
          swap(self.heap, 1, len(self.heap)-1)
          result = self.heap.pop()
          heapify(self.heap, 1, len(self.heap))
          
          return result
  
      def __str__(self):
          return str(self.heap)
  
  # 출력 코드
  priority_queue = PriorityQueue()
  
  priority_queue.insert(6)
  priority_queue.insert(9)
  priority_queue.insert(1)
  priority_queue.insert(3)
  priority_queue.insert(10)
  priority_queue.insert(11)
  priority_queue.insert(13)
  
  print(priority_queue.extract_max())
  print(priority_queue.extract_max())
  print(priority_queue.extract_max())
  print(priority_queue.extract_max())
  print(priority_queue.extract_max())
  print(priority_queue.extract_max())
  print(priority_queue.extract_max())
  ```




### 이진 탐색 트리

- 특정 노드의 왼쪽 자식 쪽 부분 트리의 노드들은 특정 노드보다 작고, 오른쪽 자식 쪽 부분 트리는의 노드들은 특정 노드보다 큰 속성을 지닌 트리

-  힙은 항상 완전 이진 트리이기 때문에 주로 배열이나 리스트로 구현했었지만, 이진 탐색 트리는 완전 이진 트리라는 보장이 없다

  - 따라서 노드 인스턴스를 생성해서 연결하는 방식으로 구현한다
  - 인스턴스 변수로 자식들의 정보만 두지 않고 부모의 정보도 넣는 식으로 주로 구현한다. 마치 더블리 링크드 리스트처럼
  - 링크드 리스트처럼 이진 탐색 트리 자체도 클래스로 정의해놓고 쓰면 더 편하다

- 이진 탐색 트리 & in-order 순회 활용

  - 이진 탐색 트리에서 in-order 순회를 사용하면 가장 작은 데이터부터 정렬해서 처리할 수 있다

- 이진 탐색 트리 삽입 방법

  - 새로운 노드 생성
  - root 노드부터 크기를 비교하면서 저장 위치할 위치 탐색
  - 찾은 위치에 새로운 노드를 연결
  - 따라서 이진 탐색 트리 삽입의 최악의 경우는 트리 높이를 h라 했을 때, O(h)이다

- 이진 탐색 트리 탐색 방법

  - root 노드에서 시작해서 해당 노드와 탐색하려는 데이터를 비교
  - 탐색하려는 데이터가 해당 노드 데이터보다 크면 오른쪽 자식으로 가서 탐색
  - 탐색하려는 데이터가 해당 노드 데이터보다 작으면 왼쪽 자식으로 가서 탐색
  - 탐색하려는 데이터를 가진 노드를 찾으면 리턴
  - 따라서 이진 탐색 트리 탐색의 최악의 경우는 트리 높이를 h라 했을 때, O(h)이다

- 이진 탐색 트리 삭제

  - 삭제하려는 노드를 탐색해서 삭제하면 됨
  - 경우1: 삭제하려는 노드가 leaf 노드이면?
    - 부모 노드가 자식 노드 연결 끊으면 됨
  - 경우2: 삭제하려는 노드가 하나의 자식 노드만 갖는 경우?
    - 삭제하려는 노드의 부모와 삭제하려는 노드의 자식을 연결해주면 됨
  - 경우3: 삭제하려는 노드가 두 개의 자식 노드를 갖는 경우?
    - 삭제하려는 노드의 오른쪽 자식 노드가 속한 부분 노드 중 가장 작은 노드를 찾는다(이를 간편하게 successor 노드라 부른다)
    - successor 노드의 데이터를 삭제하려는 노드에 복사한다
      - 여전히 이진 탐색 트리 속성은 유지된다. 왜냐면 successor 노드는 삭제하려는 노드의 오른쪽에 있었기 때문에 삭제하려는 노드의 왼쪽 자식 노드가 속한 부분 노드들 보다 크기 때문이다. 또한 successor 노드는 삭제하려는 노드의 오른쪽 자식 노드가 속한 부분 노드 중 가장 작기 때문에 삭제하려는 노드의 오른쪽 자식 노드들 역시 successor 노드보다 모두 크기 때문이다
    - successor 노드의 오른쪽 자식이 속한 부분 노드가 있다면 successor의 부모 노드(즉, 삭제하려는 노드의 오른쪽 자식 노드)와 연결한다. 그리고 successor 노드의 연결을 끊어서 삭제한다
      - 이 때, successor 노드의 왼쪽 자식은 없다. successor 노드의 왼쪽 자식이 있었다면 왼쪽 자식이 successor 노드가 되었어야 하기 때문이다
  - 경우 1~3은 모두 노드 탐색 과정이 들어가므로 O(h|h는 트리의 높이) 시간 복잡도를 가진다. 경우 3에는 successor 과정이 있기 때문에 추가적인 O(h) 시간 복잡도를 가진다. 따라서 이진 탐색 트리 삭제의 시간 복잡도는 O(h)이다

- 이진 탐색 트리 높이 h 분석

  - 만약, 이진 탐색 트리에 n개의 순차적인 값들이 입력된다면 이진 탐색 트리의 높이는 n이 된다. 이러한 트리의 상태를 트리가 편향되었다고 하고, 마치 완전 이진 트리 처럼 데이터가 입력된 경우를 균형 잡힌 트리라고 부른다
  - 위에 적어 놓은 이진 탐색 트리 연산의 시간 복잡도는 모두 트리의 높이에 비례한다. 따라서 편향된 트리는 연산이 비효율적이고 균형 잡힌 트리는 연산이 효율적이다
  - O(h)는 최악의 경우 O(n)이고, 평균적으로는 O(log n)이다. 평균적으로 O(log n)이라는 것은 데이터를 삽입할 때 모든 순서의 확률이 동일하다고 가정했을 때 나오는 수치인데, 해석학적 방법들을 동원해서 증명하면 평균적으로 O(log n)이 나온다

- ```python
  ## 이진 탐색 트리 구현 예시
  class Node:
      """이진 탐색 트리 노드 클래스"""
      def __init__(self, data):
          self.data = data
          self.parent = None
          self.right_child = None
          self.left_child = None
  
  
  def print_inorder(node):
      """주어진 노드를 in-order로 출력해주는 함수"""
      if node is not None:
          print_inorder(node.left_child)
          print(node.data)
          print_inorder(node.right_child)
  
  
  class BinarySearchTree:
      """이진 탐색 트리 클래스"""
      def __init__(self):
          self.root = None
  
  
      def delete(self, data):
          """이진 탐색 트리 삭제 메소드"""
          node_to_delete = self.search(data)  # 삭제할 노드를 가지고 온다
          parent_node = node_to_delete.parent  # 삭제할 노드의 부모 노드
  
          # 경우 1: 지우려는 노드가 leaf 노드일 때
          if node_to_delete.left_child is None and node_to_delete.right_child is None:
              if self.root is node_to_delete:
                  self.root = None
              else:  # 일반적인 경우
                  if node_to_delete is parent_node.left_child: 
                      parent_node.left_child = None
                  else:
                      parent_node.right_child = None
  
          # 경우 2: 지우려는 노드가 자식이 하나인 노드일 때:
          elif node_to_delete.left_child is None:  # 지우려는 노드가 오른쪽 자식만 있을 때:
              # 지우려는 노드가 root 노드일 때
              if node_to_delete is self.root:
                  self.root = node_to_delete.right_child
                  self.root.parent = None
              # 지우려는 노드가 부모의 왼쪽 자식일 때
              elif node_to_delete is parent_node.left_child:
                  parent_node.left_child = node_to_delete.right_child
                  node_to_delete.right_child.parent = parent_node
              # 지우려는 노드가 부모의 오른쪽 자식일 때
              else:
                  parent_node.right_child = node_to_delete.right_child
                  node_to_delete.right_child.parent = parent_node
  
          elif node_to_delete.right_child is None:  # 지우려는 노드가 왼쪽 자식만 있을 때:
              # 지우려는 노드가 root 노드일 때
              if node_to_delete is self.root:
                  self.root = node_to_delete.left_child
                  self.root.parent = None
              # 지우려는 노드가 부모의 왼쪽 자식일 때
              elif node_to_delete is parent_node.left_child:
                  parent_node.left_child = node_to_delete.left_child
                  node_to_delete.left_child.parent = parent_node
              # 지우려는 노드가 부모의 오른쪽 자식일 때
              else:
                  parent_node.right_child = node_to_delete.left_child
                  node_to_delete.left_child.parent = parent_node
  
          # 경우 3: 지우려는 노드가 2개의 자식이 있을 때
          successor = self.find_min(node_to_delete.right_child)  # 삭제하려는 노드의 successor 노드 받아오기
  
          node_to_delete.data = successor.data  # 삭제하려는 노드의 데이터에 successor의 데이터 저장
  
          # successor 노드 트리에서 삭제
          if successor is successor.parent.left_child:  # successor 노드가 오른쪽 자식일 때
              successor.parent.left_child = successor.right_child
          else:  # successor 노드가 왼쪽 자식일 때
              successor.parent.right_child = successor.right_child        
          
          if successor.right_child is not None:  # successor 노드가 오른쪽 자식이 있을 떄
              successor.right_child.parent = successor.parent
  
      @staticmethod
      def find_min(node):
          """(부분)이진 탐색 트리의 가장 작은 노드 리턴"""
          # 코드를 쓰세요
          temp = node  # 탐색 변수. 파라미터 node로 초기화
  
          # temp가 node를 뿌리로 갖는 부분 트리에서 가장 작은 노드일 때까지 왼쪽 자식 노드로 간다
          while temp.left_child is not None:
              temp = temp.left_child      
  
          return temp  
  
  
      def search(self, data):
          """이진 탐색 트리 탐색 메소드, 찾는 데이터를 갖는 노드가 없으면 None을 리턴한다"""
          temp = self.root  # 탐색 변수. root 노드로 초기화
      
          # 원하는 데이터를 갖는 노드를 찾을 때까지 돈다
          while temp is not None:
              # 원하는 데이터를 갖는 노드를 찾으면 리턴
              if data == temp.data:
                  return temp
              # 원하는 데이터가 노드의 데이터보다 크면 오른쪽 자식 노드로 간다
              if data > temp.data:
                  temp = temp.right_child
              # 원하는 데이터가 노드의 데이터보다 작으면 왼쪽 자식 노드로 간다
              else:
                  temp = temp.left_child
      
          return None # 원하는 데이터가 트리에 없으면 None 리턴
  
  
      def insert(self, data):
          """이진 탐색 트리 삽입 메소드"""
          new_node = Node(data)  # 삽입할 데이터를 갖는 노드 생성
  
          # 트리가 비었으면 새로운 노드를 root 노드로 만든다
          if self.root is None:
              self.root = new_node
              return
  
          # 코드를 쓰세요
          temp = self.root  # 저장하려는 위치를 찾기 위해 사용할 변수. root 노드로 초기화한다
  
          # 원하는 위치를 찾아간다
          while temp is not None:
              if data > temp.data:  # 삽입하려는 데이터가 현재 노드 데이터보다 크다면
                  # 오른쪽 자식이 없으면 새로운 노드를 현재 노드 오른쪽 자식으로 만듦
                  if temp.right_child is None:
                      new_node.parent = temp
                      temp.right_child = new_node
                      return
                  # 오른쪽 자식이 있으면 오른쪽 자식으로 간다
                  else:
                      temp = temp.right_child
              else:  # 삽입하려는 데이터가 현재 노드 데이터보다 작다면
                  # 왼쪽 자식이 없으면 새로운 노드를 현재 노드 왼쪽 자식으로 만듦
                  if temp.left_child is None:
                      new_node.parent = temp
                      temp.left_child = new_node
                      return
                  # 왼쪽 자식이 있다면 왼쪽 자식으로 간다
                  else:
                      temp = temp.left_child
              
  
      def print_sorted_tree(self):
          """이진 탐색 트리 내의 데이터를 정렬된 순서로 출력해주는 메소드"""
          print_inorder(self.root)  # root 노드를 in-order로 출력한다
  
  
  # 빈 이진 탐색 트리 생성
  bst = BinarySearchTree()
  
  # 데이터 삽입
  bst.insert(7)
  bst.insert(11)
  bst.insert(9)
  bst.insert(17)
  bst.insert(8)
  bst.insert(5)
  bst.insert(19)
  bst.insert(3)
  bst.insert(2)
  bst.insert(4)
  bst.insert(14)
  
  # 자식이 두 개 다 있는 노드 삭제
  bst.delete(7)
  bst.delete(11)
  
  bst.print_sorted_tree()
  ```

- 이진 탐색 트리로 구현하는 추상 자료형 딕셔너리, 세트
  - 트리를 만들 때 쓰는 Node의 인스턴스 변수로 data 대신 key와 value를 사용해서 구현하면 된다
  - 이진 탐색 트리로 구현된 딕셔너리는 데이터 삽입, 탐색, 삭제 과정 모두 O(h)이고, 평균적으로 O(log n)이다
  - 이전에 해시 테이블 자료 구조를 배울 때 이를 통해 딕셔너리와 세트를 구현한다고 배웠다. 해시 테이블로 구현된 딕셔너리와 세트는 데이터 삽입, 탐색, 삭제 연산 모두 평균적으로 O(1)이고, 최악의 경우 O(n)이었다
    - 따라서 해시 테이블로 구현된 딕셔너리와 세트가 이진 탐색 트리로 구현된 딕셔너리와 세트보다 모든 연산에서 빠르다
    - 딕셔너리와 세트는 순서를 생각하지 않는 자료형이지만 때로는 순서가 필요할 때도 있다
    - 이진 탐색 트리는 순서를 저장할 수 있는 형태이므로(in-order 순회) 정렬된 상태의 딕셔너리나 세트 자료형을 사용하고 싶다면, 연산의 효율성을 포기하고 이진 탐색 트리로 구현하면 된다



### 그래프

- 연결 관계 데이터를 저장할 수 있는 자료 구조
  - cf) 선형적 관계(배열, 링크드 리스트), 계층적 관계(트리)
  - ex) 위치 데이터, 사회 관계 연결망 데이터, 통신 데이터(컴퓨터 간 연결), 생물 데이터(유전자, 단백질 등의 상호 작용)
- 그래프 기본 개념
  - 그래프의 기본 단위도 Node
    - 링크드 리스트나 트리와는 다르게 모든 Node가 동등함. Head나 Root 노드 없음
  - 노드 간 관계는 Edge로 표현
    - 한 노드가 갖고 있는 Edge 수는 차수라고 함
    - Edge에 방향이 있으면 유향 그래프, 없으면 무향 그래프라고 함
      - 유향 그래프에서는 입력과 출력을 나누어서 차수를 계산함
    - Edge에 가중치를 줄 수도 있는데, 이는 가중치 그래프라 함
  - Edge로 연결된 길을 경로라고 함
    - 경로의 연결이 이어져서 노드로 다시 돌아올 수 있는 경우는 Cycle이라고 함
    - 연결 노드 사이의 모든 Edge 가중치의 합을 경로의 거리라고 함
- Edge 구현
  - 인접 행렬 방식
    - Edge 관계도를 따로 행렬(2차원 배열)로 표현하겠다는 방식
  - 인접 리스트 방식
    - 노드의 인스턴스 변수로 리스트를 두고 그 안에 인접 노드를 넣어서 Edge를 표현하겠다는 방식
  - 인접 행렬 vs 인접 리스트
    - V(vertex): 모든 노드들의 집합. 모든 노드의 수로 사용하기도 함
    - E(Edge): 모든 엣지들의 집합. 모든 엣지의 수로 사용하기도 함
    - 그래프에서는 점근 표기법을 사용할 때 n 대신 V와 E를 사용한다
    - 공간 복잡도
      - 인접 행렬은 행렬을 다 만들어 놓고 표시하기 때문에 O(V^2^)이다
      - 인접 리스트는 O(V+E)이다. E가 V^2^에 비례하기 때문에 최악의 경우(모든 노드가 연결되어 있는 경우) O(V^2^)라고 볼 수도 있지만 O(V+E)로 주로 표현함
    - 두 노드의 연결 여부를 확인하는 시간 복잡도
      - 인접 행렬은 O(1)
      - 인접 리스트는 최악의 경우(모든 노드가 연결되어 있는 경우) O(V)
    - 한 노드에 연결된 모든 노드들을 확인하는 시간 복잡도
      - 인접 행렬은 O(V)
      - 인접 리스트는 최악의 경우(모든 노드가 연결되어 있는 경우) O(V)이기는 하나 보통은 이보다 적은 시간이 걸림
    - 따라서 주로 어떤 연산을 하는 지 따져서 용도에 맞게 선택하면 됨
- 그래프 탐색
  - BFS(Breadth First Search)
    - Queue 사용
    - 방문한 노드를 표시하는 로직을 둔다
    - 큐에 첫번째 노드를 넣는다
    - 큐에서 노드를 꺼내서 해당 노드에 방문한 후 다음에 방문해야 할 노드(인접 노드들)를 큐에 넣는다
    - 더이상 큐에서 꺼낼 노드가 없으면 탐색 완료
  - DFS(Depth First Search)
    - Stack 사용
    - 방문한 노드를 표시하는 로직을 둔다
      - BFS와는 다르게 3가지 상태를 표시한다. 방문하지 않은 노드(0), 스택에 이미 넣은 노드(1), 방문한 노드(2)
      - 상태를 2가지로 두면 사이클이 생겼을 때 BFS가 되는 것을 방지하려고 하는 것 같다
    - 스택에 첫번째 노드를 넣는다
    - 스택에서 노드를 꺼내서 해당 노드에 방문한 후 해당 노드의 인접 노드들 중 방문되지 않은 노드들을 스택에 넣는다
    - 더이상 스택에서 꺼낼 노드가 없으면 탐색 완료
- 최단 경로 알고리즘
  - 최단 경로를 찾는 알고리즘은 다양하다. 그래프의 종류가 다양하기 때문이다
    - 방향, 가중치, 사이클, 음수 엣지 존재 여부 등
  - 비가중치 그래프
    - predecessor BFS 알고리즘
      - BFS를 할 때 이전 노드를 인스턴스 변수 predecessor에 기록해 놓고, 최단 경로를 찾고자 하는 경로의 마지막 노드에서 predecessor를 백트래킹해가면서 경로를 찾는다
  - 가중치 그래프
    - 다익스트라 알고리즘
      - 인스턴스 변수 distance, predecessor, complete
        - distance
          - 시작점으로부터의 거리를 나타내는 변수. 초기값은 아주 큰 수로 둔다
        - predecessor
          - 이전 노드를 기록하는 변수
        - complete
          - 모든 경우를 체크했는지 기록하는 변수
      - Relaxation 과정
        - Complete된 노드를 포함하는 엣지가 아닌 새로운 엣지를 발견했을 때, 노드에 기록된 distance 값을 최선의 값으로 업데이트 해주는 과정
      - 과정 설명
        1. 시작점의 distance를 0으로 바꾼다
        2. complete 처리 되지 않은 노드 중 distance가 가장 작은 노드를 선택한다
        3. 해당 노드의 인접 노드를 돌면서 각 엣지에 Relaxation 과정을 수행한다
        4. 해당 노드를 complete 처리한다
        5. 모든 노드가 complete 처리 될 때까지 2~4과정 반복
      - 원리
        - 다음으로 complete 처리할 노드를 선택할 때 complete 처리 되지 않은 노드 중 distance가 가장 작은 노드를 선택하는 것이 핵심이다
        - complete 처리 되지 않은 노드 중 distance가 가장 작은 노드를 선택했으므로, 다른 노드들에 의해 relax 되어 distance가 수정될 가능성은 없다. 왜냐하면 남은 노드를 거쳐서 선택된 노드로 오는 경로는 distance가 더 클 수 밖에 없기 때문이다. 그 경로는 선택된 노드보다 distance가 크거나 같을 것이고 새로운 엣지의 가중치도 더해줘야 하니까
        - 위의 내용은 음수의 가중치를 갖는 그래프에서 다익스트라 알고리즘을 사용할 수 없는 이유이기도 하다
